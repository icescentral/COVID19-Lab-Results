{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Lab Results\n",
    "Written by: Branson Chen <br>\n",
    "Last modified: 20200710"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<a href='#Overview'>Overview</a><br>\n",
    "<a href='#Input-variables'>Input variables</a><br>\n",
    "<a href='#Importing-data'>Importing data</a><br>\n",
    "<a href='#Text-analysis'>Text analysis</a><br>\n",
    "\n",
    "- <a href='#Algorithm-description'>Algorithm description</a><br>\n",
    "- <a href='#Initial-processing'>Initial processing</a><br>\n",
    "- <a href='#Assign-results'>Assign results</a><br>\n",
    "\n",
    "<a href='#Final-output'>Final output</a><br>\n",
    "<a href='#Testing-and-validation'>Testing and validation</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This script first imports a SAS dataset based on the input variables provided, and then fields are decoded/renamed.\n",
    "- Next, the text is cleaned (clean function) and then tokenized (tokenize function).\n",
    "- Relevant labels are then assigned to the tokens (assign_labels function).\n",
    "- The labelled tokens are then interpreted using an in-house algorithm (interpret function).\n",
    "- All of the information from the previous step is then collapsed to give one result per virus per test (process_result function), and unidentified virus/test types are filled in based on observation codes and testrequest codes.\n",
    "- Lastly, the results are converted to a single character per virus type (char_output function) and then output in a csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input path and filename (should be .sas7bdat file)\n",
    "input_path = ''\n",
    "input_filename = ''\n",
    "\n",
    "#name of patientid variable in input dataset, will be renamed as 'patientid'\n",
    "input_patientid_var = 'ikn'\n",
    "\n",
    "#output additional columns\n",
    "#0 = just virus flags, 1 = with key columns, 2 = with ALL columns\n",
    "output_flag = 1\n",
    "\n",
    "#output filename (should be .csv file)\n",
    "output_filename = 'output.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#import sas file (COMPRESS=BINARY MAY NOT WORK WITH READ_SAS; COMPRESS=YES|CHAR DOES NOT WORK WITH READ_SAS)\n",
    "df_raw=pd.read_sas(input_path+input_filename)\n",
    "\n",
    "#decode strings (np objects)\n",
    "df_raw.loc[:, df_raw.dtypes == np.object] = df_raw.loc[:, df_raw.dtypes == np.object].apply(lambda x: x.str.decode('UTF-8'))\n",
    "df_raw.fillna('', inplace=True)\n",
    "print('# of records:',len(df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy(deep = True)\n",
    "\n",
    "#rename variables\n",
    "df = df.rename(columns={input_patientid_var:'patientid','fillerordernumber':'fillerordernumberid',\n",
    "                       'observationvalue':'value','observationsubid':'subid'})\n",
    "#keep key cols\n",
    "key_cols = ['patientid', 'ordersid', 'interpretationvalue', 'fillerordernumberid', \n",
    "            'reportinglaborgname', 'performinglaborgname', 'observationdatetime', \n",
    "            'testrequestcode', 'observationcode', 'observationreleasets', \n",
    "            'observationresultstatus', 'subid', 'value']\n",
    "df = df[key_cols]\n",
    "\n",
    "#set exclude_flag based on observationresultstatus = W\n",
    "df_W = df.loc[df['observationresultstatus'] == 'W', ['ordersid', 'observationcode', 'value']]\n",
    "df_excl = df[['ordersid', 'observationcode', 'value']].reset_index().merge(df_W, how='inner').set_index('index')\n",
    "df['exclude_flag'] = 'N'\n",
    "df.loc[df.index.isin(df_excl.index),['exclude_flag']] = 'Y'\n",
    "print(df['exclude_flag'].value_counts())\n",
    "\n",
    "#set exclude_flag based on DO NOT TRANSMIT code\n",
    "# DNT_text = '<p1:MicroOrganism xmlns:p1=\"http://www.ssha.ca\"><p1:Code>99999999999</p1:Code><p1:Text>Do Not Transmit</p1:Text><p1:CodingSystem>HL79905</p1:CodingSystem></p1:MicroOrganism>'\n",
    "# df_DNT = df.loc[df['value'] == DNT_text, ['ordersid', 'observationcode','observationreleasets']]\n",
    "# df_excl2 = df[['ordersid', 'observationcode', 'observationreleasets']].reset_index().merge(df_DNT, how='inner').set_index('index')\n",
    "# df.loc[df.index.isin(df_excl2.index),['exclude_flag']] = 'Y'\n",
    "# print(df['exclude_flag'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine which observations need to be concatenated\n",
    "group_cols = ['ordersid', 'fillerordernumberid', 'reportinglaborgname', \n",
    "              'testrequestcode', 'observationcode', 'observationreleasets', 'observationresultstatus']\n",
    "df_gp_subid = df.reset_index().groupby(group_cols).agg({'index':tuple, 'subid':tuple}).reset_index()\n",
    "df_gp_subid = df_gp_subid.rename(columns={'index':'original_indexes'})\n",
    "\n",
    "#only concatenate ones where there are more than two subids, all the subids are numbers and contains 1\n",
    "df_to_concat = df_gp_subid[df_gp_subid['subid'].apply(lambda x: all([subid.isdigit() for subid in x]) and len(x) > 2 and '1' in x)]\n",
    "concat_indexes = [i for tup in df_to_concat['original_indexes'] for i in tup]\n",
    "\n",
    "#concatenate based on subid\n",
    "df_gp_concat = df[df.index.isin(concat_indexes)].reset_index()\n",
    "df_gp_concat['subid'] = df_gp_concat['subid'].apply(int)\n",
    "df_gp_concat = df_gp_concat.sort_values(by = group_cols+['subid']).groupby(group_cols)\n",
    "df_gp_concat = df_gp_concat.agg({'index': tuple,\n",
    "                   'value': lambda x: ' '.join(map(str, x))}).reset_index()\n",
    "\n",
    "#add on records that were not concatenated\n",
    "df_gp = df.loc[~df.index.isin(concat_indexes), group_cols+['value']].reset_index()\n",
    "df_gp['index'] = df_gp['index'].apply(lambda x: (x,))\n",
    "df_gp = pd.concat([df_gp_concat, df_gp], sort=False).rename(columns={'index':'original_indexes'})\n",
    "\n",
    "print('# of TEST RESULTS:', len(df_gp))\n",
    "\n",
    "#cleanup\n",
    "del df_W\n",
    "del df_excl\n",
    "del df_gp_subid\n",
    "del df_to_concat\n",
    "del df_gp_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean punctuation, xml field, numbers, other text\n",
    "puncs = [';', ':', ',', '.', '-', '_', '/', '(', ')', '[', ']', '{', '}', '<', '>', '*', '#', '?', '.', '+', \n",
    "        'br\\\\', '\\\\br', '\\\\e\\\\', '\\\\f\\\\', '\\\\t\\\\', '\\\\', \"'\", '\"', '=']\n",
    "terms_to_space = ['detected', 'by', 'positive', 'parainfluenza', 'accession']\n",
    "nums_following = ['date', 'telephone', 'tel', 'phone', 'received', 'collected',  \n",
    "                 'result', 'on', 'at', '@', 'approved', 'final', 'time', 'number']\n",
    "strings_to_replace = {'non detected':'not detected', 'npot detected':'not detected', 'nor detected':'not detected',\n",
    "                      'mot detected':'not detected', 'n0t detected':'not detected',\n",
    "                      'covid 19 virus not interpretation detected':'covid 19 virus interpretation not detected',\n",
    "                      'presumptive interpretation':'interpretation presumptive',\n",
    "                      'preliminary interpretation':'interpretation preliminary',\n",
    "                      'covid 19 not detected and covid 19 detected':'covid 19 detected and covid 19 not detected',\n",
    "                      'virusnot':'virus not', 'prevuous':'previous'}\n",
    "date_id_patterns = [r'\\d{2,4} \\d{2} \\d{2,4} ', r'\\d{4} \\d{2} ', r'\\d{4}h ', \n",
    "                   r' \\d{0,2}[a-z]{0,2}\\d{5,}[a-z]{0,1}', r' [a-z]{0,2}\\d{1,3}[a-z]{1,3}\\d{4,}[a-z]{0,1}']\n",
    "\n",
    "def clean(value):\n",
    "    cleaned = value.lower()\n",
    "\n",
    "    #clean xml field, only keep text field surrounded with 'p1 text'\n",
    "    pattern = r'(<p1:microorganism xmlns)(.+)(<p1:text>.+</p1:text>)(.+)(</p1:microorganism>)'\n",
    "    while re.search(pattern, cleaned):\n",
    "        cleaned = re.sub(pattern, r'\\g<3>', cleaned)\n",
    "    \n",
    "    #surround terms with spaces (some terms found stuck together)\n",
    "    for t in terms_to_space:\n",
    "        cleaned = cleaned.replace(t, ' ' + t + ' ')\n",
    "    \n",
    "    #replace punctuation with space\n",
    "    for punc in puncs:\n",
    "        cleaned = cleaned.replace(punc, ' ')\n",
    "\n",
    "    #remove consecutive spaces\n",
    "    while '  ' in cleaned:\n",
    "        cleaned = cleaned.replace('  ', ' ')\n",
    "    \n",
    "    cleaned = cleaned.strip()     \n",
    "    \n",
    "    #remove numbers after certain terms\n",
    "    for term in nums_following:\n",
    "        pattern = term + r' \\d{1,4}'\n",
    "        \n",
    "        while re.search(pattern, cleaned):\n",
    "            cleaned = re.sub(pattern, term, cleaned)\n",
    "            \n",
    "    #remove more dates and ids\n",
    "    for pattern in date_id_patterns:\n",
    "        while re.search(pattern, cleaned):\n",
    "            cleaned = re.sub(pattern, '', cleaned)\n",
    "    \n",
    "    #remove numbers at the end\n",
    "    while len(cleaned) > 0 and (cleaned[-1].isdigit() or cleaned[-1] == ' '):\n",
    "        cleaned = cleaned[:-1]\n",
    "    \n",
    "    #remove \"no\" at the end\n",
    "    while cleaned.endswith(' no') or cleaned == 'no':\n",
    "        cleaned = cleaned[:-3]\n",
    "    \n",
    "    #fix certain strings\n",
    "    for k, v in strings_to_replace.items():\n",
    "        cleaned = cleaned.replace(k, v)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize values using nltk\n",
    "def tokenize(value):\n",
    "    tokenized = nltk.word_tokenize(value)\n",
    "   \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign labels for useful tokens based on some dictionaries and exclusions\n",
    "easy_virus_dict = {'v_adenovirus':['aden'], 'v_bocavirus':['boca', 'bocca'], 'v_coronavirus':['coro', 'cora'],\n",
    "                   'v_entero_rhino':['enterol', 'enterov', 'entervir', 'rhino', 'rhini'], 'v_hmv':['metap']}\n",
    "hard_virus_dict = {'v_rsv':['rsv'], 'v_flu':['nflu', 'flue'], 'v_para':['parai', 'pata', 'parta'],\n",
    "                   'v_covid':['cov', 'sars', 'orf1', 'orfl', 'or1lab']}\n",
    "indirect_matches_dict = {'r_pos': ['posi'], \n",
    "                         'r_neg': ['neg', 'naeg', 'neag'],  \n",
    "                         'r_ind': ['indeter', 'eterminate', 'inconclu',\n",
    "                                   'equivocal', 'uninterpret', 'unresolved'],\n",
    "                         'r_can': ['cancel', 'forward', 'incorrect', 'duplicate', 'mislabel', 'recollect'],\n",
    "                         'r_rej': ['reject', 'inval', 'leak', 'unable', 'insuffic', \n",
    "                                   'spill', 'inapprop', 'nsq', 'poor'],\n",
    "                         'presumptive': ['presump', 'prelim', 'possi'], \n",
    "                         'retest': ['retest']}\n",
    "direct_matches_dict = {'r_pos': ['detected', 'pos', 'deteced', 'postive', 'organism'],\n",
    "                       'r_neg': ['no', 'not'],\n",
    "                       'r_ind': ['ind'],\n",
    "                       'r_pen': ['pending', 'progress', 'follow', 'ordered', 'reordered'],\n",
    "                       'r_can': ['sent', 'send', 'redirected'],\n",
    "                       'presumptive': ['single', 'possible', 'probable'],\n",
    "                       'xml': ['p1'], \n",
    "                       'reset': ['deleted','anesthesiologist'],\n",
    "                       'stop': ['specific', 'required', 'error', 'copy', 'see', \n",
    "                                'note', 'stability', 'changed', 'recollect', 'moh', 'if'],\n",
    "                       'final': ['interpretation', 'interpetation', 'interp', 'pretation',\n",
    "                                 'final', 'overall', 'updated', 'corrected', 'proved'],\n",
    "                       'skip': ['reason', 'identify'],\n",
    "                       'connecting': ['targets', 'tagets', 'target', 'screen', 'presence', 'as', 'real',\n",
    "                                      'is', 'of', 'in', '1', '2', '3', '4', 'a', 'b', 'c', \n",
    "                                      '229e', 'nl63', 'hku1', 'oc43', '19', '2019', 'low',\n",
    "                                      'biosafety', 'hazard', 'has', 'been', 'for', 'changed', 'identified', \n",
    "                                      'result', 'other', 'testing', 'using', 'to', 'from', 'tested',\n",
    "                                      'phl', 'phol', 'phlo', 'new', 'request', 'lab', 'will']}\n",
    "test_type_dict = {'t_oth': ['eia', 'rapid', 'immunoassay', 'ict', 'immunochromatographic', 'antigen'], \n",
    "                  't_pcr': ['multiplex', 'naat', 'nat', 'pcr', 'rrt', 'gene', 'rna', 'gen', \n",
    "                            'reverse', 'polymerase', 'chain', 'simplexa']}\n",
    "\n",
    "def assign_labels(tokenized):\n",
    "    tokenized_length = len(tokenized)\n",
    "    useful = [None]*tokenized_length #store same list length of tokens and update each accordingly\n",
    "    \n",
    "    for counter, token in enumerate(tokenized):\n",
    "        \n",
    "        #skip if already assigned\n",
    "        if useful[counter]:\n",
    "            continue\n",
    "        \n",
    "        ###easy viruses dictionary (non-exact matching)\n",
    "        for virus, patterns in easy_virus_dict.items():\n",
    "            if any([pattern in token for pattern in patterns]):\n",
    "                useful[counter] = virus\n",
    "                break\n",
    "\n",
    "        #extra rhino/entero rule (exact matching)\n",
    "        if token in ('rhino', 'entero'):\n",
    "            useful[counter] = 'v_entero_rhino'\n",
    "\n",
    "        ###hard viruses dictionary (non-exact matching)\n",
    "        \n",
    "        #COVID19 \n",
    "        elif any([pattern in token for pattern in hard_virus_dict['v_covid']])\\\n",
    "        and not any([pattern in token for pattern in ('ecov', 'cove')])\\\n",
    "        and 'mers' not in tokenized[counter-3:counter]:\n",
    "            useful[counter] = 'v_covid'\n",
    "            \n",
    "        #e/envelope/n/nucleocapsid gene\n",
    "        elif token in ('e', 'envelope', 'n', 'nucleocapsid') and (tokenized_length > counter+1)\\\n",
    "        and tokenized[counter+1] == 'gene':\n",
    "            useful[counter:counter+2] = ['v_covid', 'connecting']\n",
    "        \n",
    "        #rdrp gene\n",
    "        elif token == 'rdrp' and (tokenized_length > counter+1)\\\n",
    "        and tokenized[counter+1] == 'gene' and 'v_coronavirus' not in useful[:counter]:\n",
    "            useful[counter:counter+2] = ['v_covid', 'connecting']\n",
    "        \n",
    "        #extra rule for seasonal coronavirus, if preceded by novel or followed by 19/disease/cov/sars/2\n",
    "        elif any([pattern in token for pattern in easy_virus_dict['v_coronavirus']]):\n",
    "            if 'nove' in tokenized[counter-1] or tokenized[counter-1] == 'nivel':\n",
    "                useful[counter-1:counter+1] = ['v_covid', 'connecting']\n",
    "                \n",
    "            covid_extra = [] #extra terms\n",
    "            look_forward = 3 #how many terms to look forward for\n",
    "            max_forward = min(counter+look_forward, tokenized_length-1) #limit if record is too short\n",
    "            covid_extra = [(tokenized[covid_pos], covid_pos) for covid_pos in range(counter+1, max_forward+1)\\\n",
    "                       if any([pattern in tokenized[covid_pos] for pattern in ('19', 'disea', 'cov', 'sars')]\\\n",
    "                              +[tokenized[covid_pos] == '2'])]\n",
    "            \n",
    "            #assign range of relevant tokens as virus\n",
    "            if len(covid_extra) > 0:\n",
    "                last_pos = max([x[1] for x in covid_extra])\n",
    "                useful[counter:last_pos+1] = ['v_covid']+['connecting']*(last_pos-counter)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #PARA\n",
    "        elif any([pattern in token for pattern in hard_virus_dict['v_para']]+[token == 'para'])\\\n",
    "        and tokenized[counter-1] != 'haemophilus':\n",
    "            para_extra = []\n",
    "            look_forward = 5\n",
    "            max_forward = min(counter+look_forward, tokenized_length-1)\n",
    "            para_extra = [(tokenized[para_pos], para_pos) for para_pos in range(counter+1, max_forward+1)\\\n",
    "                              if tokenized[para_pos] in ('1','2','3','4')]\n",
    "            \n",
    "            if len(para_extra) > 0:\n",
    "                last_pos = max([x[1] for x in para_extra])\n",
    "                para_nums = [x[0] for x in para_extra]\n",
    "                useful[counter:last_pos+1] = ['v_para_' + '_'.join(para_nums)]+['connecting']*(last_pos-counter)\n",
    "            else:\n",
    "                useful[counter] = 'v_para'\n",
    "\n",
    "        #FLU\n",
    "        elif any([pattern in token for pattern in hard_virus_dict['v_flu']]+[token in ('flu', 'inf')])\\\n",
    "        and tokenized[counter-1] != 'haemophilus':\n",
    "            flu_extra = []\n",
    "            look_forward = 4\n",
    "            max_forward = min(counter+look_forward, tokenized_length-1)\n",
    "            \n",
    "            for flu_pos in range(counter+1, max_forward+1):\n",
    "                if tokenized[flu_pos] in ('a','b') or 'h1' in tokenized[flu_pos] or 'h3' in tokenized[flu_pos]:\n",
    "                    flu_extra.append((tokenized[flu_pos], flu_pos))\n",
    "                elif 'flu' in tokenized[flu_pos]: #to deal with influenza a influenza b\n",
    "                    break\n",
    "                \n",
    "            if len(flu_extra) > 0:\n",
    "                last_pos = max([x[1] for x in flu_extra])\n",
    "                flu_types = [x[0] for x in flu_extra]\n",
    "                if 'a' in flu_types and 'b' in flu_types:\n",
    "                    useful[counter:last_pos+1] = ['v_flu_a_b']+['connecting']*(last_pos-counter)\n",
    "                elif 'b' in flu_types:\n",
    "                    useful[counter:last_pos+1] = ['v_flu_b']+['connecting']*(last_pos-counter)\n",
    "                elif any(['h1' in f for f in flu_types]) and any(['h3' in f for f in flu_types]):\n",
    "                    useful[counter:last_pos+1] = ['v_flu_a_h1_h3']+['connecting']*(last_pos-counter)\n",
    "                elif any(['h1' in f for f in flu_types]):\n",
    "                    useful[counter:last_pos+1] = ['v_flu_a_h1']+['connecting']*(last_pos-counter)\n",
    "                elif any(['h3' in f for f in flu_types]):\n",
    "                    useful[counter:last_pos+1] = ['v_flu_a_h3']+['connecting']*(last_pos-counter)\n",
    "                elif 'a' in flu_types:\n",
    "                    useful[counter:last_pos+1] = ['v_flu_a']+['connecting']*(last_pos-counter)                                                                  \n",
    "            elif token.endswith('aa'):\n",
    "                useful[counter] = 'v_flu_a'\n",
    "            elif token.endswith('ab'):\n",
    "                useful[counter] = 'v_flu_b'\n",
    "            else:\n",
    "                useful[counter] = 'v_flu'\n",
    "\n",
    "        #RSV\n",
    "        elif any([pattern in token for pattern in hard_virus_dict['v_rsv']]):\n",
    "            rsv_extra = []\n",
    "            look_forward = 2\n",
    "            max_forward = min(counter+look_forward, tokenized_length-1) \n",
    "            rsv_extra = [(tokenized[rsv_pos], rsv_pos) for rsv_pos in range(counter+1, max_forward+1)\\\n",
    "                       if tokenized[rsv_pos] in ('a','b')]\n",
    "                \n",
    "            if len(rsv_extra) > 0:\n",
    "                last_pos = max([x[1] for x in rsv_extra])\n",
    "                rsv_types = [x[0] for x in rsv_extra]\n",
    "                if 'a' in rsv_types and 'b' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_a_b']+['connecting']*(last_pos-counter)\n",
    "                elif 'a' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_a']+['connecting']*(last_pos-counter)\n",
    "                elif 'b' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_b']+['connecting']*(last_pos-counter)\n",
    "            else:\n",
    "                useful[counter] = 'v_rsv'\n",
    "\n",
    "        elif (tokenized_length > counter+2) and ((token.startswith('resp')\\\n",
    "        and tokenized[counter+1].startswith('syn') and tokenized[counter+2].startswith('vi'))\\\n",
    "        or (token == 'r' and tokenized[counter+1] == 's' and tokenized[counter+2] == 'v')):\n",
    "            rsv_extra = []\n",
    "            look_forward = 4\n",
    "            max_forward = min(counter+look_forward, tokenized_length-1) \n",
    "            rsv_extra = [(tokenized[rsv_pos], rsv_pos) for rsv_pos in range(counter+3, max_forward+1)\\\n",
    "                       if tokenized[rsv_pos] in ('a','b')]\n",
    "\n",
    "            if len(rsv_extra) > 0:\n",
    "                last_pos = max([x[1] for x in rsv_extra])\n",
    "                rsv_types = [x[0] for x in rsv_extra]\n",
    "                if 'a' in rsv_types and 'b' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_a_b']+['connecting']*(last_pos-counter)\n",
    "                elif 'a' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_a']+['connecting']*(last_pos-counter)\n",
    "                elif 'b' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_b']+['connecting']*(last_pos-counter)\n",
    "            else:\n",
    "                useful[counter:counter+3] = ['v_rsv', 'connecting', 'connecting']\n",
    "        \n",
    "        #UNKNOWN VIRUS\n",
    "        elif (token.startswith('vir') or token.startswith('viu')):\n",
    "            #extra rule for virus culture\n",
    "            if (tokenized_length > counter+2) and tokenized[counter+1].startswith('cult')\\\n",
    "            and 'request' in tokenized[counter+2]:\n",
    "                useful[counter:counter+3] = ['connecting']*3\n",
    "            elif (tokenized_length > counter+1) and tokenized[counter+1].startswith('cult'):\n",
    "                useful[counter:counter+2] = ['t_oth']*2\n",
    "            else:\n",
    "                useful[counter] = 'v_unk'\n",
    "        \n",
    "        #extra terms to treat as an \"unknown virus\" for purpose of algorithm\n",
    "        elif token in ('by','further','specimen','specimens','test','sample','considered'):\n",
    "            useful[counter] = 'v_unk'\n",
    "        \n",
    "    # loop over the record again\n",
    "    for counter, token in enumerate(tokenized):\n",
    "        \n",
    "        #skip if already assigned\n",
    "        if useful[counter]:\n",
    "            continue\n",
    "\n",
    "        #culture tests  \n",
    "        if token.startswith('cult') and not ((tokenized_length > counter+1) and 'request' in tokenized[counter+1]):\n",
    "            useful[counter] = 't_oth'\n",
    "\n",
    "        #additional \"direct\" tests\n",
    "        elif token == 'direct' and (tokenized_length > counter+1):\n",
    "            if tokenized[counter+1] in ('kit', 'enzyme', 'test', 'testing', 'eia', 'antigen', 'ict'):\n",
    "                useful[counter:counter+2] = ['t_oth']*2\n",
    "            elif tokenized[counter+1] in ('influenza',):\n",
    "                useful[counter] = 't_oth'\n",
    "        \n",
    "        #condition for mention of pos/neg\n",
    "        elif token in ('negative','neg','positive','pos','detected','organism') and (tokenized_length > counter+1)\\\n",
    "        and ((tokenized[counter-1] in ('a','original','or','level','of','the')\n",
    "              and tokenized[counter+1] in ('test','result','covid'))\n",
    "             or tokenized[counter+1] in ('or','swab','results','to','p1','contact','workers','retest')):\n",
    "            useful[counter-1:counter+2] = [None]*3\n",
    "        elif token in ('negative','neg','positive','pos','detected','organism') and (tokenized_length > 1)\\\n",
    "        and (tokenized[counter-2] in ('previous','previously','contact','worker','depot','targets')\n",
    "             or tokenized[counter-1] in ('previous','previously','known','unit','first','second',\n",
    "                                         'needs','need','requires','considered','swab','if',\n",
    "                                         'depot','employee','gram')):\n",
    "            useful[counter-1:counter+1] = [None]*2\n",
    "            \n",
    "        #condition for word before no\n",
    "        elif token == 'no' and (tokenized[counter-1] in ('by','lab','specimen','accession','sample')\n",
    "                                or any([pattern in tokenized[counter-1] for pattern in ('out','break')]))\\\n",
    "        and tokenized[counter+1:counter+2] != ['virus']:\n",
    "            useful[counter-1:counter+1] = [None]*2\n",
    "            \n",
    "        #condition for word after no\n",
    "        elif token == 'no' and (tokenized_length > counter+1)\\\n",
    "        and tokenized[counter+1] in ('specimen','reportable','done','gene','fever','answer','result','longer',\n",
    "                                     'media','liquid','sample','swab','nasopharyngeal','record','fluid',\n",
    "                                     'patient','second','results','testing','grh'):\n",
    "            useful[counter] = 'r_can'\n",
    "\n",
    "        #condition for due to\n",
    "        elif tokenized[counter:counter+2] == ['due','to'] and 'new' not in tokenized[counter+2:counter+4]:\n",
    "            useful[counter:counter+2] = ['stop']*2\n",
    "        \n",
    "        #condition for not test/been\n",
    "        elif token == 'not' and (tokenized_length > counter+1)\\\n",
    "        and tokenized[counter+1] in ('test','been','suspicious','validated','the'):\n",
    "            useful[counter:counter+2] = ['skip']*2\n",
    "        \n",
    "        #condition for word following not\n",
    "        elif token == 'not' and (tokenized_length > counter+1) and \\\n",
    "        tokenized[counter+1] in ('tested','tessted','perform','performed','process','processed', \n",
    "                                 'transmit','suitable','done','doen','be','reported','received', \n",
    "                                 'match','needed','labelled','available','symptomatic','forwared',\n",
    "                                 'met','specified','indicated','returned','sufficient','given',\n",
    "                                 'valid','required','able','needed','contain','ordered','recieved',\n",
    "                                 'labeled','a','provided','appropriate'):\n",
    "            useful[counter:counter+2] = ['r_can']*2\n",
    "        \n",
    "        #condition for does not\n",
    "        elif token == 'not' and tokenized[counter-1] in ('does','did','please','done'):\n",
    "            useful[counter-1:counter+1] = ['skip']*2\n",
    "        \n",
    "        #condition for ordered in error\n",
    "        elif 'ordered' in token and tokenized[counter+1:counter+3] == ['in', 'error']:\n",
    "            useful[counter:counter+3] = ['r_can']*3\n",
    "      \n",
    "        #condition for target rna\n",
    "        elif tokenized[counter:counter+2] in (['target','rna'],['patient','disregard']):\n",
    "            useful[counter:counter+2] = ['end']*2\n",
    "        \n",
    "        #condition for previous\n",
    "        elif 'previous' in token and ('reported' in tokenized[counter+1:counter+3] or\n",
    "                                      tokenized[counter+1:counter+3] in (['report','of'],\n",
    "                                                                         ['reports','of'],\n",
    "                                                                         ['result','of'],\n",
    "                                                                         ['covid','19'])):\n",
    "            useful[counter:counter+3] = ['end']*3\n",
    "        \n",
    "        #unable and indeterminate\n",
    "        elif tokenized[counter:counter+5] == ['unable','to','be','completed','indeterminate']:\n",
    "            useful[counter:counter+4] = ['connecting']*4\n",
    "        \n",
    "        else:\n",
    "            #indirect_matches dictionary\n",
    "            for term, patterns in indirect_matches_dict.items():\n",
    "                if any([pattern in token for pattern in patterns]):\n",
    "                    useful[counter] = term\n",
    "                    break\n",
    "                    \n",
    "            #direct_matches dictionary\n",
    "            for term, patterns in direct_matches_dict.items():\n",
    "                if any([pattern == token for pattern in patterns]):\n",
    "                    useful[counter] = term\n",
    "                    break\n",
    "                    \n",
    "            #test_type dictionary\n",
    "            for test, patterns in test_type_dict.items():\n",
    "                if any([pattern == token for pattern in patterns]):\n",
    "                    useful[counter] = test\n",
    "                    break\n",
    "        \n",
    "    return useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the useful_tokens field, this interpret function sequentially \"reads\" the terms. It picks up virus/result/test terms and they are held in a \"bundle\" (virus, result, test). There are also multiple modifiers that affect the way that the algorithm processes the terms. These modifiers are: final (flag to take highest priority later on), presumptive (change pos to pre), end (end reading early or skip the next save), and skip (skip the 'save when virus switches' rule once). Any time a bundle is saved, the bundle (except for test type) and the final/presumptive modifiers are cleared. If a save occurs with incomplete information, the virus defaults to an unknown virus, result defaults to negative, and test defaults to unknown test. Whenever a save happens, all of the previous tokens+labels that were read are considered to be a \"segment\".\n",
    "<br>\n",
    "- First, the xml field is processed if there is one. If a relevant virus is found, it is treated as a positive and the bundle is saved.\n",
    "- Next, the algorithm will go through the labelled tokens one by one. There are different conditions for storing terms and saving the bundle when encountering a virus, a result, a special term, or an irrelevant (unlabelled) term.\n",
    "    - Viruses: A relevant virus is always kept. If the virus switches, save the bundle (note: can be affected by skip modifier). If the same virus is read, save the bundle only if there is a result as well. An unknown virus is only kept if there is no current virus.\n",
    "    - Results: A clear result (ind, neg, pos) is kept with hierarchy ind > neg > pos such that a neg/ind can overwrite a positive if it's close together (e.g., \"not detected\" becomes a neg). An unclear result (rej, can, pen) is only kept if there is no current result with hierarchy rej > can > pen. If there is already a previous result and a neg/ind is encounter, save the bundle.\n",
    "    - Special terms:\n",
    "        - Final: Modifier to add flag when saving to specify whether it is a final result, which takes higher priority over all others in the process_result function. Save if there is a current virus and current result. Clear the current result.\n",
    "        - Presumptive: Modifier to change positive (r_pos) into presumptive-positive (r_pre).\n",
    "        - End: Modifier to skip the next save. Save if there is a current virus and current result. Stop the reading if there are any results.\n",
    "        - Skip: Modifier to skip the 'save when virus switches' once. This is only reset when a virus switches and the current virus is skipped. Save if there is a current virus and current result. Clear the bundle.\n",
    "        - Reset: Clear bundle without saving.\n",
    "        - Stop: Save if there is a current virus and current result. Clear the bundle.        \n",
    "    - Irrelevant terms: If two irrelevant terms (Nones) are read in a row, save the bundle if there is both a current result and virus. Also save the bundle if there is a virus and the past segment had another virus (virus_counter > 1; normally viruses tested are listed in a mpx or pcr assay). Otherwise, clear the bundle without saving and reset all the counter variables (i.e., start a new segment).\n",
    "    - If the sentence ends before hitting two Nones, save any result and save the bundle if there is a virus and the past segment had another virus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpret text to get initial results\n",
    "def interpret(useful):\n",
    "    \n",
    "    def save(b):\n",
    "        #presumptive modifier\n",
    "        if b[1] == 'r_pos' and modifier[1]:\n",
    "            b[1] = 'r_pre'\n",
    "\n",
    "        #end modifier (skips a save)\n",
    "        if not modifier[2]:\n",
    "            output.append([b[0] if b[0] else 'v_unk', \n",
    "                           b[1] if b[1] else 'r_neg', \n",
    "                           b[2] if b[2] else 't_unk', \n",
    "                           modifier[0]]) #final modifier\n",
    "        \n",
    "        b[0] = None\n",
    "        b[1] = None\n",
    "        modifier[0:3] = [False, False, False]\n",
    "        return\n",
    "    \n",
    "    sentence = useful[:]\n",
    "    output = []\n",
    "    \n",
    "    #bundle for current virus/result/test\n",
    "    #0 = virus, 1 = result, 2 = test\n",
    "    bundle = [None, None, None]\n",
    "    \n",
    "    #modifiers\n",
    "    #0 = final, 1 = presumptive, 2 = end, 3 = skip\n",
    "    modifier = [False, False, False, False]\n",
    "    \n",
    "    none_counter = 0 #counter for hitting consecutive irrelevant words\n",
    "    virus_counter = 0 #counter for different viruses in same segment\n",
    "    \n",
    "    #xml field processing\n",
    "    xml_pos = [i for i, x in enumerate(sentence) if x == 'xml']\n",
    "    num = len(xml_pos)//2\n",
    "    for i in range(num):\n",
    "        xml_start_pos = xml_pos[i*2]\n",
    "        xml_end_pos = xml_pos[i*2+1]\n",
    "        for j in range(xml_start_pos, xml_end_pos + 1):\n",
    "            if sentence[j] and sentence[j].startswith('v_') and sentence[j] != 'v_unk':\n",
    "                bundle[0] = sentence[j]\n",
    "                bundle[1] = 'r_pos'\n",
    "                save(bundle)\n",
    "    \n",
    "    #add result to output if result in first 3 words\n",
    "    if len(sentence) > 3 and not any(['v_' in s for s in sentence[0:3] if s]):\n",
    "        for s in sentence[0:3]:\n",
    "            if s and 'r_' in s:\n",
    "                output.append(['v_unk', s, 't_unk', False])\n",
    "                break\n",
    "            elif s and s == 'connecting':\n",
    "                pass\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    #if there is mention of retest but no final result, take earliest one as final\n",
    "    if 'retest' in sentence and 'final' not in sentence:\n",
    "        modifier[0] = True\n",
    "    \n",
    "    #loop on words in sentence\n",
    "    for word in sentence:\n",
    "        \n",
    "        if word: #relevant term\n",
    "            none_counter = 0 #restart counter\n",
    "            \n",
    "            #set current virus \n",
    "            if word.startswith('v_'):\n",
    "                #different virus\n",
    "                if word != 'v_unk' and word != bundle[0]:\n",
    "                    #save current result if hitting a different virus\n",
    "                    if bundle[0] and bundle[0] != 'v_unk':\n",
    "                        #skip modifier\n",
    "                        if modifier[3]:\n",
    "                            modifier[3] = None #reset skip modifier\n",
    "                            bundle[1] = None\n",
    "                        else:\n",
    "                            save(bundle)\n",
    "                            virus_counter += 1 #increase counter if different virus in segment     \n",
    "                    bundle[0] = word\n",
    "                #same virus\n",
    "                elif word != 'v_unk' and word == bundle[0]:\n",
    "                    #save current result if there is one\n",
    "                    if bundle[1]:\n",
    "                        save(bundle)\n",
    "                    bundle[0] = word\n",
    "                #only set to general virus if there's no current virus\n",
    "                elif word == 'v_unk' and not bundle[0]:\n",
    "                    bundle[0] = word\n",
    "                \n",
    "            #set current result\n",
    "            elif word.startswith('r_'):\n",
    "                if word == 'r_ind':\n",
    "                    if bundle[1]: \n",
    "                        save(bundle)\n",
    "                    bundle[1] = word\n",
    "                elif word == 'r_neg' and bundle[1] not in ('r_ind',):\n",
    "                    if bundle[1]: \n",
    "                        save(bundle)\n",
    "                    bundle[1] = word\n",
    "                elif word == 'r_pos' and bundle[1] not in ('r_ind', 'r_neg'):\n",
    "                    bundle[1] = word\n",
    "\n",
    "                elif word in ('r_rej', 'r_can', 'r_pen') and bundle[1] not in ('r_ind', 'r_neg', 'r_pos'):\n",
    "                    if word == 'r_rej':\n",
    "                        bundle[1] = word\n",
    "                    elif word == 'r_can' and bundle[1] not in ('r_rej',):\n",
    "                        bundle[1] = word\n",
    "                    elif word == 'r_pen' and bundle[1] not in ('r_rej', 'r_can'):\n",
    "                        bundle[1] = word\n",
    "                \n",
    "            #set current test\n",
    "            elif word.startswith('t_'):\n",
    "                bundle[2] = word\n",
    "            \n",
    "            #final modifier\n",
    "            elif word == 'final':\n",
    "                if bundle[0] and bundle[1]:\n",
    "                    save(bundle)\n",
    "                modifier[0] = True\n",
    "                bundle[1] = None #reset result\n",
    "            \n",
    "            #presumptive modifier\n",
    "            elif word == 'presumptive':\n",
    "                modifier[1] = True\n",
    "            \n",
    "            #end modifier/word\n",
    "            elif word == 'end':\n",
    "                if bundle[0] and bundle[1]:\n",
    "                    save(bundle)\n",
    "                modifier[0:3] = [False, False, True] #end modifier skips next save\n",
    "                #end early only if there is already result\n",
    "                if len(output) > 0:\n",
    "                    return output\n",
    "            \n",
    "            #skip modifier\n",
    "            elif word == 'skip':\n",
    "                if bundle[0] and bundle[1]:\n",
    "                    save(bundle)\n",
    "                modifier[3] = True\n",
    "                bundle[0] = None\n",
    "                bundle[1] = None\n",
    "            \n",
    "            #stop word\n",
    "            elif word == 'stop':\n",
    "                if bundle[0] and bundle[1]:\n",
    "                    save(bundle)\n",
    "                modifier[0:3] = [False, False, False]\n",
    "                bundle[0] = None\n",
    "                bundle[1] = None           \n",
    "                \n",
    "            #reset word\n",
    "            elif word == 'reset':\n",
    "                modifier[0:3] = [False, False, False]\n",
    "                bundle[0] = None\n",
    "                bundle[1] = None\n",
    "            \n",
    "        else: #word is None\n",
    "            none_counter += 1\n",
    "            \n",
    "            if none_counter == 2: #can change threshold\n",
    "                #save if there is current virus and result\n",
    "                if bundle[0] and bundle[1]:\n",
    "                    save(bundle)\n",
    "                #save the last virus if multiple were listed\n",
    "                elif bundle[0] and bundle[0] != 'v_unk' and virus_counter > 1:\n",
    "                    if modifier[3]:\n",
    "                        modifier[3] = None #reset skip modifier\n",
    "                    else:\n",
    "                        save(bundle)\n",
    "                #reset\n",
    "                none_counter = 0 \n",
    "                virus_counter = 0\n",
    "                bundle[0] = None\n",
    "                bundle[1] = None\n",
    "                modifier[0:3] = [False, False, False]\n",
    "                \n",
    "    #if there is still a remaining result\n",
    "    if bundle[1]: \n",
    "        save(bundle)\n",
    "    \n",
    "    #if there is an extra virus listed at the end\n",
    "    elif bundle[0] and bundle[0] != 'v_unk' and virus_counter > 1 and not modifier[3]:\n",
    "        save(bundle)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using reference excel to assign 89 LOINCs + 11 LOINCs to virus and test type\n",
    "#added COVID19 LOINCs\n",
    "xlsx_filename = 'COVID19_Resp_codes_20200413.xlsx'\n",
    "mappings = {'--':'unk', 'culture':'cult', 'other':'oth', 'entero_rhino_D68':'entero_rhino'}\n",
    "\n",
    "df_loincs = pd.read_excel(xlsx_filename, sheet_name='Resp_LOINCs')\n",
    "df_loincs_covid = pd.read_excel(xlsx_filename, sheet_name='COVID19_LOINCs')\n",
    "df_loincs = df_loincs.append(df_loincs_covid)\n",
    "\n",
    "#cleaning the categories to match previously defined ones\n",
    "df_loincs = df_loincs.replace(mappings)\n",
    "df_loincs['Virus_to_assign'] = df_loincs['Virus_to_assign'].apply(lambda x: 'coronavirus' if 'corona' in x else x)\n",
    "df_loincs['Virus_to_assign'] = df_loincs['Virus_to_assign'].apply(lambda x: 'v_' + x)\n",
    "df_loincs['Test_to_assign'] = df_loincs['Test_to_assign'].apply(lambda x: 't_' + x)\n",
    "\n",
    "#assign LOINCs to virus and test type\n",
    "loincs_by_v = {}\n",
    "loincs_by_t = {}\n",
    "for index, row in df_loincs.iterrows():\n",
    "    loincs_by_v.setdefault(row['Virus_to_assign'], [])\n",
    "    loincs_by_v[row['Virus_to_assign']].append(row['LOINCs'])\n",
    "    loincs_by_t.setdefault(row['Test_to_assign'], [])\n",
    "    loincs_by_t[row['Test_to_assign']].append(row['LOINCs'])\n",
    "\n",
    "#remove the unk ones\n",
    "del loincs_by_v['v_unk']\n",
    "del loincs_by_t['t_unk']\n",
    "    \n",
    "#use reference excel to assign 19 TR codes to virus and test type\n",
    "#added COVID19 TR codes\n",
    "df_tr_codes = pd.read_excel(xlsx_filename, sheet_name='Resp_TRs')\n",
    "df_tr_covid = pd.read_excel(xlsx_filename, sheet_name='COVID19_TRs')\n",
    "df_tr_codes = df_tr_codes.append(df_tr_covid)\n",
    "\n",
    "#cleaning the categories to match previously defined ones\n",
    "df_tr_codes = df_tr_codes.replace(mappings)\n",
    "df_tr_codes['Virus_to_assign'] = df_tr_codes['Virus_to_assign'].apply(lambda x: 'coronavirus' if 'corona' in x else x)\n",
    "df_tr_codes['Virus_to_assign'] = df_tr_codes['Virus_to_assign'].apply(lambda x: 'v_' + x)\n",
    "df_tr_codes['Test_to_assign'] = df_tr_codes['Test_to_assign'].apply(lambda x: 't_' + x)\n",
    "\n",
    "#assign LOINCs to virus and test type\n",
    "tr_codes_by_v = {}\n",
    "tr_codes_by_t = {}\n",
    "for index, row in df_tr_codes.iterrows():\n",
    "    tr_codes_by_v.setdefault(row['Virus_to_assign'], [])\n",
    "    tr_codes_by_v[row['Virus_to_assign']].append(row['TRs'])\n",
    "    tr_codes_by_t.setdefault(row['Test_to_assign'], [])\n",
    "    tr_codes_by_t[row['Test_to_assign']].append(row['TRs'])\n",
    "    \n",
    "#remove the unk ones\n",
    "del tr_codes_by_v['v_unk']\n",
    "del tr_codes_by_t['t_unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign more details to v_unk or t_unk based on LOINC and TR code\n",
    "# group by test type and then type of virus, remove duplicates\n",
    "loinc_exclusions = ['21026-0','22634-0','22635-7','22636-5','22637-3','22638-1','22639-9',\n",
    "                    '31208-2','33882-2','35265-8','47526-9','49049-0','55752-0','59465-5',\n",
    "                    '664-3','XON10007-3','XON10011-5','XON10337-4','XON11913-1','XON12721-7',\n",
    "                    'XON13543-4']\n",
    "\n",
    "def process_result(tokens, testrequestcode, observationcode, results):\n",
    "    dd = {}\n",
    "    \n",
    "    ###extra conditions\n",
    "    \n",
    "    #LOINC exclusions\n",
    "    if observationcode in loinc_exclusions:\n",
    "        return dd \n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        #delete all results for irrelevant phrases\n",
    "        if tokens[i:i+5] in (['swab', 'is', 'required', 'for', 'both'],\n",
    "                             ['is', 'unable', 'to', 'go', 'until']):\n",
    "            return dd\n",
    "        #make presumptive-positive if test is investigational\n",
    "        if tokens[i:i+3] in (['not', 'been', 'established'], \n",
    "                             ['is', 'considered', 'investigational'], \n",
    "                             ['a', 'retrospective', 'review']):\n",
    "            for r in results:\n",
    "                if (r[0] == 'v_covid' or r[0] == 'v_unk') and r[1] == 'r_pos':\n",
    "                    r[1] = 'r_pre' \n",
    "        #change negative to pending if there are results to follow\n",
    "        if tokens[i:i+3] == ['to', 'follow', 'tested']:\n",
    "            for r in results:\n",
    "                if r[1] in ('r_neg','r_can','r_rej') and not r[3]:\n",
    "                    r[1] = 'r_pen'      \n",
    "    \n",
    "    #change negative to indeterminate for indeterminate multiplex\n",
    "    if 'indeterminate' in tokens[0:3] and len(set([v for (v,r,t,f) in results])) > 5 and 'all' not in tokens[3:8]:\n",
    "        for r in results:\n",
    "            if r[1] == 'r_neg' and not r[3]:\n",
    "                r[1] = 'r_ind'\n",
    "      \n",
    "    ###determine virus or test based on LOINC or TR\n",
    "    v_from_loinc = [loinc_vir for loinc_vir, loincs in loincs_by_v.items() if observationcode in loincs]\n",
    "    v_from_tr = [tr_codes_vir for tr_codes_vir, tr_codes in tr_codes_by_v.items() if testrequestcode in tr_codes]\n",
    "    t_from_loinc = [loinc_test for loinc_test, loincs in loincs_by_t.items() if observationcode in loincs]\n",
    "    t_from_tr = [tr_codes_test for tr_codes_test, tr_codes in tr_codes_by_t.items() if testrequestcode in tr_codes]\n",
    "    \n",
    "    #determine if there are any final/interpretation results\n",
    "    viruses_with_final = [v for (v,r,t,f) in results if r in ('r_pos', 'r_pre', 'r_ind', 'r_neg') and f]\n",
    "    results_final = results\n",
    "    #remove the non-final/interpretation results for viruses with final/interpretation\n",
    "    for vf in viruses_with_final:\n",
    "        results_final = [(v,r,t,f) for (v,r,t,f) in results if not (v in (vf, 'v_unk') and not f)]\n",
    "        \n",
    "    for v, r, t, f in results_final:\n",
    "        #fill in unknown virus\n",
    "        if v == 'v_unk':\n",
    "            if len(v_from_loinc) > 0:\n",
    "                v = v_from_loinc[0]\n",
    "            elif len(v_from_tr) > 0:\n",
    "                v = v_from_tr[0]\n",
    "        \n",
    "        #fill in unknown test\n",
    "        if t == 't_unk':\n",
    "            if len(t_from_loinc) > 0:\n",
    "                t = t_from_loinc[0]\n",
    "            elif len(t_from_tr) > 0:\n",
    "                t = t_from_tr[0]\n",
    "            \n",
    "        #fill in pcr if there is a pcr term in text\n",
    "        if t == 't_unk' and 'pcr' in tokens: \n",
    "            t = 't_pcr'\n",
    "        \n",
    "        #remove unknown virus results\n",
    "        if v != 'v_unk':\n",
    "            v, r, t = v[2:], r[2:], t[2:]\n",
    "            #all tests that aren't pcr are oth\n",
    "            #t = t if t == 'pcr' else 'oth'\n",
    "            \n",
    "            #ASSUME EVERYTHING PCR FOR COVID DATASET\n",
    "            t = 'pcr'\n",
    "        \n",
    "            dd.setdefault(t, [])\n",
    "            \n",
    "            #compiling results with hierarchy: S (presumptive positive) > P (positive) > I (indeterminate) \n",
    "            #                                  > N (negative) > D (pending) > R (invalid) > C (cancelled) \n",
    "            same_vir = False\n",
    "            for i in range(len(dd[t])):\n",
    "                if v == dd[t][i][0]:\n",
    "                    same_vir = True\n",
    "                    if r == 'pre':\n",
    "                        dd[t][i] = (v,r)\n",
    "                    elif r == 'pos' and dd[t][i][1] not in ('pre',):\n",
    "                        dd[t][i] = (v,r)\n",
    "                    elif r == 'ind' and dd[t][i][1] not in ('pre', 'pos'):\n",
    "                        dd[t][i] = (v,r)\n",
    "                    elif r == 'neg' and dd[t][i][1] not in ('pre', 'pos', 'ind'):\n",
    "                        dd[t][i] = (v,r)\n",
    "                    elif r == 'pen' and dd[t][i][1] not in ('pre', 'pos', 'ind', 'neg'):\n",
    "                        dd[t][i] = (v,r)\n",
    "                    elif r == 'rej' and dd[t][i][1] not in ('pre', 'pos', 'ind', 'neg', 'pen'):\n",
    "                        dd[t][i] = (v,r)\n",
    "                    elif r == 'can':\n",
    "                        pass\n",
    "            if not same_vir:\n",
    "                dd[t].append((v,r))\n",
    "        \n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create output as character value for each virus and test type\n",
    "result_char = {'pre':'S', 'pos': 'P', 'ind':'I', 'neg':'N', 'pen':'D', 'can':'C', 'rej':'R'}\n",
    "\n",
    "def char_output(results, ind):\n",
    "    \n",
    "    #loop through each test type and virus\n",
    "    for t, pairs in results.items(): #need to update if there are multiple test types\n",
    "        for v, r in pairs:\n",
    "            if v in ('adenovirus', 'bocavirus', 'coronavirus', 'entero_rhino', 'hmv', 'covid'):\n",
    "                df_results[v][ind] = result_char[r]\n",
    "            \n",
    "            elif v.startswith('para'):\n",
    "                df_results['para'][ind] = result_char[r]\n",
    "                    \n",
    "            elif v.startswith('flu'):\n",
    "                df_results['flu'][ind] = result_char[r]   \n",
    "                if '_a' in v:\n",
    "                    df_results['flu_a'][ind] = result_char[r]\n",
    "                if '_h1' in v:\n",
    "                    df_results['flu_a_h1'][ind] = result_char[r]\n",
    "                if '_h3' in v:\n",
    "                    df_results['flu_a_h3'][ind] = result_char[r]\n",
    "                if '_b' in v:\n",
    "                    df_results['flu_b'][ind] = result_char[r]\n",
    "            \n",
    "            elif v.startswith('rsv'):\n",
    "                df_results['rsv'][ind] = result_char[r]\n",
    "                if '_a' in v:\n",
    "                    df_results['rsv_a'][ind] = result_char[r]\n",
    "                if '_b' in v:\n",
    "                    df_results['rsv_b'][ind] = result_char[r]\n",
    "                   \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#make copy of df\n",
    "df_unique = df_gp.copy(deep = True)\n",
    "\n",
    "#clean text\n",
    "df_unique[\"cleaned_value\"] = df_unique[\"value\"].apply(clean)\n",
    "\n",
    "#group by unique records (org, TR code, Obs code, cleaned text) and store original indexes as tuple\n",
    "df_unique = df_unique.reset_index()\n",
    "groupby_vars = ['reportinglaborgname', 'testrequestcode', 'observationcode', 'cleaned_value']\n",
    "df_unique = df_unique.groupby(groupby_vars).agg({'value': 'count', \n",
    "                                                 'original_indexes': lambda x: tuple([i for tup in x for i in tup])}).reset_index()\n",
    "df_unique = df_unique.rename(columns={'value':'count'})\n",
    "\n",
    "df_unique = df_unique.sort_values(by=['count'], ascending=False).reset_index(drop=True)\n",
    "print('unique records after cleaning:', len(df_unique))\n",
    "\n",
    "#tokenize\n",
    "df_unique[\"cleaned_tokenized_value\"] = df_unique[\"cleaned_value\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign labels using dictionary\n",
    "df_unique[\"useful_tokens\"] = df_unique[\"cleaned_tokenized_value\"].apply(assign_labels)\n",
    "\n",
    "#interpret the labelled tokens\n",
    "df_unique[\"initial_results\"] = df_unique[\"useful_tokens\"].apply(interpret)\n",
    "\n",
    "#fill in unknown viruses based on LOINC or TR code, roll up results to one test type\n",
    "final_results = []\n",
    "for i in range(len(df_unique)):\n",
    "    final_results.append(process_result(df_unique[\"cleaned_tokenized_value\"][i],\n",
    "                                        df_unique[\"testrequestcode\"][i], df_unique[\"observationcode\"][i], \n",
    "                                        df_unique[\"initial_results\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate results to 1-character format\n",
    "col_virus = ['covid', 'adenovirus', 'bocavirus', 'coronavirus', 'flu', 'flu_a', 'flu_a_h1', 'flu_a_h3', 'flu_b',\n",
    "         'entero_rhino', 'hmv', 'para', 'rsv', 'rsv_a', 'rsv_b']\n",
    "cols = [v for v in col_virus]\n",
    "\n",
    "#create empty df to fill in results\n",
    "df_results = pd.DataFrame(index=np.arange(len(df_unique)), columns=cols)\n",
    "\n",
    "#fill in results\n",
    "for i in range(len(df_unique)):\n",
    "    char_output(final_results[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracker for unique records (some records may be marked as new if clean function changes)\n",
    "\n",
    "#initialize tracker\n",
    "try:\n",
    "    f = open('record_tracker.pkl')\n",
    "    f.close()\n",
    "except FileNotFoundError:\n",
    "    df_tracker = pd.DataFrame(columns=['filename', 'reportinglaborgname', 'testrequestcode', 'observationcode', 'cleaned_value'])\n",
    "    df_tracker.to_pickle(\"./record_tracker.pkl\")\n",
    "    print('CREATING RECORD TRACKER FILE')\n",
    "    \n",
    "#read tracker\n",
    "df_tracker = pd.read_pickle('./record_tracker.pkl')\n",
    "\n",
    "#RESET TRACKER\n",
    "#df_tracker = df_tracker.iloc[0:0]\n",
    "\n",
    "df_tracker_orig = df_tracker[['reportinglaborgname', 'testrequestcode', 'observationcode', 'cleaned_value']].copy(deep = True)\n",
    "df_tracker_delta = df_unique[['reportinglaborgname', 'testrequestcode', 'observationcode', 'cleaned_value']].copy(deep = True)\n",
    "\n",
    "#set difference\n",
    "df_tracker_delta = pd.concat([df_tracker_delta, df_tracker_orig, df_tracker_orig], ignore_index=True).drop_duplicates(keep=False)\n",
    "print('Original tracker length:', len(df_tracker_orig))\n",
    "print('Delta tracker length:', len(df_tracker_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate output for checking results\n",
    "int_output_cols = ['count', 'reportinglaborgname', 'testrequestcode', 'observationcode', 'cleaned_value']\n",
    "df_unique[int_output_cols].join(df_results).to_csv('intermediate_output.csv')\n",
    "df_unique[int_output_cols][df_unique.index.isin(df_tracker_delta.index)].join(df_results).to_csv('intermediate_output_delta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_create_output = df_unique[['original_indexes']].join(df_results)\n",
    "output = [None]*len(df)\n",
    "\n",
    "#order results based on original_indexes\n",
    "for row in df_create_output.itertuples():\n",
    "    for i in row[1]: #original_indexes\n",
    "        output[i] = row[2:]\n",
    "\n",
    "df_output = pd.DataFrame(output, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL DATASET TO OUTPUT\n",
    "if output_flag == 0:\n",
    "    df[['exclude_flag']].join(df_output).reset_index().to_csv(output_filename, index=False)\n",
    "elif output_flag == 1:\n",
    "    df.join(df_output).reset_index().to_csv(output_filename, index=False)\n",
    "elif output_flag == 2:\n",
    "    df_raw.join(df[['exclude_flag']].join(df_output)).reset_index().to_csv(output_filename, index=False)\n",
    "else:\n",
    "    print('PLEASE ENTER ONE OF THE FOLLOWING OPTIONS FOR OUTPUT_FLAG IN THE FIRST CELL: 0, 1, 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_output['covid'].value_counts()\n",
    "#df_output.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP RUNNING HERE IF MANUAL REVIEW IS NOT DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINALIZE THE RECORD TRACKER (only run when you are satisfied with the review process)\n",
    "#add filename\n",
    "df_tracker_delta['filename'] = input_filename\n",
    "\n",
    "#add the delta\n",
    "df_tracker = pd.concat([df_tracker, df_tracker_delta], sort=False, ignore_index=True)\n",
    "\n",
    "#save file\n",
    "df_tracker.to_pickle(\"./record_tracker.pkl\")\n",
    "print('Records in tracker:', len(df_tracker))\n",
    "\n",
    "#cleanup\n",
    "del df_raw\n",
    "del df\n",
    "del df_gp\n",
    "del df_unique\n",
    "del df_output\n",
    "del df_tracker\n",
    "del df_tracker_orig\n",
    "del df_tracker_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test a string\n",
    "test_string = r'''\n",
    "\n",
    "'''\n",
    "\n",
    "test_clean = clean(test_string)\n",
    "print('--', test_clean)\n",
    "test_useful = assign_labels(tokenize(test_clean))\n",
    "print('--', test_useful)\n",
    "test_interpret = interpret(test_useful)\n",
    "print('--', test_interpret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check specific record based on intermediate_output index\n",
    "\n",
    "x = \n",
    "print('ordersid:', df_raw[\"ordersid\"][df_unique[\"original_indexes\"][x][0]])\n",
    "print('cleaned tokenized value:', list(df_unique[\"cleaned_tokenized_value\"][x]))\n",
    "print('useful tokens:', list(df_unique[\"useful_tokens\"][x]))\n",
    "print('initial results:', list(df_unique[\"initial_results\"][x]))\n",
    "print('final results:', final_results[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test word in all words\n",
    "all_words = list(set([word for sentence in df_unique[\"cleaned_tokenized_value\"] for word in sentence]))\n",
    "\n",
    "test_word = ''\n",
    "print('All unique words that contain \"' + test_word + '\":',\n",
    "      [word for word in all_words if test_word in word])\n",
    "print('All instances of exactly \"' + test_word + '\":', \n",
    "      len([True for c in df_unique[\"cleaned_tokenized_value\"] if test_word in c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check preceding words and frequencies\n",
    "d_preceding = {}\n",
    "word = ''\n",
    "for i in range(len(df_unique)):\n",
    "    for j in range(len(df_unique[\"cleaned_tokenized_value\"][i])):\n",
    "        if df_unique[\"cleaned_tokenized_value\"][i][j] == word and len(df_unique[\"cleaned_tokenized_value\"][i]) > 1:\n",
    "            next_term = df_unique[\"cleaned_tokenized_value\"][i][j-1]\n",
    "            d_preceding.setdefault(next_term, 0)\n",
    "            d_preceding[next_term] += 1\n",
    "print(d_preceding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check following words and frequencies\n",
    "d_following = {}\n",
    "word = ''\n",
    "for i in range(len(df_unique)):\n",
    "    for j in range(len(df_unique[\"cleaned_tokenized_value\"][i])):\n",
    "        if df_unique[\"cleaned_tokenized_value\"][i][j] == word and len(df_unique[\"cleaned_tokenized_value\"][i]) > j+1:\n",
    "            next_term = df_unique[\"cleaned_tokenized_value\"][i][j+1]\n",
    "            d_following.setdefault(next_term, 0)\n",
    "            d_following[next_term] += 1\n",
    "print(d_following)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
