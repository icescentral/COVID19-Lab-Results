{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "equal-assets",
   "metadata": {},
   "source": [
    "# COVID-19 Lab Results\n",
    "Written by: Branson Chen, Danson Lou, Gulam Mohammed, Vicky Zhang, Harrison Zhang <br>\n",
    "Last modified: 20250508"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-replacement",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "#### 1. Please make sure your input file is sorted by \"ordersid\".  \n",
    "#### 2. In the \"Function Definition\" , when importing the input file please make sure you have selected the correct datetime format.\n",
    "#### 3. Please go to \"Parameter Definition\" to define all of the required parameters.  After doing so, you can run all script. \n",
    "#### 4. The code also generates two more csv files: intermediate_output and intermediate_output_delta. You can commend it out if you don't need them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-injury",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<a href='#Overview'>Overview</a><br>\n",
    "<a href='#Import library'>Import library</a><br>\n",
    "<a href='#Function-definition'>Function definition</a><br>\n",
    "<a href='#Process-Data'>Process data</a><br>\n",
    "\n",
    "- <a href='#Parameter-Definition'>Parameter definiton</a><br>\n",
    "- <a href='#Process-Data-in-Batches'>Process Data in Batches</a><br>\n",
    "- <a href='#Combine-Batch-Outputs-to-the-Final-Output'>Combine Batch Outputs to the Final Output</a><br>\n",
    "- <a href='#Remove-Batch-Outputs'>Remove Batch Outputs</a><br>\n",
    "\n",
    "<a href='#Algorithm description'>Algorithm description</a><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-amendment",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-limitation",
   "metadata": {},
   "source": [
    "- This script first imports a SAS/CSV file based on the input variables provided, and then fields are decoded/renamed.\n",
    "- Next, the text is cleaned (clean function) and then tokenized (tokenize function).\n",
    "- Relevant labels are then assigned to the tokens (assign_labels function).\n",
    "- The labelled tokens are then interpreted using an in-house algorithm (interpret function).\n",
    "- All of the information from the previous step is then collapsed to give one result per virus per test (process_result function), and unidentified virus/test types are filled in based on observation codes and testrequest codes.\n",
    "- Lastly, the results are converted to a single character per virus type (char_output function) and then output in a csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-elimination",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat.pyreadstat\n",
    "import nltk\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "##IntegrationPoint01_Open\n",
    "##IntegrationPoint01_Close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-device",
   "metadata": {},
   "source": [
    " # Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processBatch(batch, last_batch_cut_index, batchSize, input_path, input_filename, input_patientid_var, output_filename, output_flag = 1):\n",
    "    \n",
    "    def find_current_cutpoint():\n",
    "\n",
    "        def get_ordersid_at_index(index): \n",
    "            # return the ordersid of the [index+1]th row. \n",
    "            # if the index is same or larger than the range of dataset, it will return False\n",
    "            if input_filename.endswith('.csv'):\n",
    "                ##IntegrationPoint06_Open\n",
    "                row_df = pd.read_csv(input_path+input_filename,\n",
    "                        skiprows=index, nrows=1, header=0,\n",
    "                        names=pd.read_csv(input_path+input_filename, nrows=1).columns.values)\n",
    "                ##IntegrationPoint06_Close\n",
    "            elif input_filename.endswith('.sas7bdat'):\n",
    "                row_df, _ = pyreadstat.read_sas7bdat(filename_path=input_path+input_filename, \n",
    "                                     row_limit=1, row_offset=index)\n",
    "            else:\n",
    "                raise Exception(\"file is not csv or sas7bdat\")\n",
    "            return row_df.iloc[0]['ordersid'] if len(row_df) == 1 else False\n",
    "        \n",
    "        current_file_cutpoint = (int(last_batch_cut_index/batch_size) + 1) * batch_size\n",
    "        current_last_ordersid = get_ordersid_at_index(current_file_cutpoint - 1)\n",
    "        if not current_last_ordersid: # already reached end of file\n",
    "            return current_file_cutpoint\n",
    "        while True:\n",
    "            current_file_cutpoint += 1\n",
    "            current_cutpoint_ordersid = get_ordersid_at_index(current_file_cutpoint - 1)\n",
    "            if not current_cutpoint_ordersid: # reached end of dataset\n",
    "                return current_file_cutpoint - 1 # reached end of file\n",
    "            elif current_cutpoint_ordersid != current_last_ordersid:\n",
    "                return current_file_cutpoint - 1 # found the row with different ordersid\n",
    "            \n",
    "    #if you want to use this code:\n",
    "    current_batch_cut_index = find_current_cutpoint()\n",
    "    \n",
    "    #if you want to implement your solution, and don't need find_current_cutpoint funtion:\n",
    "#    current_batch_cut_index = (batch+1) * batchSize\n",
    "    \n",
    "    ##IntegrationPoint02_Open\n",
    "    #load data for csv file\n",
    "    if input_filename.endswith('.csv'): \n",
    "        df = pd.read_csv(input_path+input_filename,\n",
    "                        skiprows=last_batch_cut_index, nrows=current_batch_cut_index-last_batch_cut_index, header=0,\n",
    "                        names=pd.read_csv(input_path+input_filename, nrows=1).columns.values, \n",
    "                        dtype={input_patientid_var: 'string',\n",
    "                               'observationsubid': 'string',\n",
    "                               'fillerordernumber': 'object',\n",
    "                               'observationcode': 'object',\n",
    "                               'observationdatetime': 'object',\n",
    "                               'observationreleasets': 'object',\n",
    "                               'observationresultstatus': 'object',\n",
    "                               'observationvalue': 'object',\n",
    "                               'ordersid': 'int64',\n",
    "                               'performinglaborgname': 'object',\n",
    "                               'reportinglaborgname': 'object',\n",
    "                               'testrequestcode': 'object'})\n",
    "        \n",
    "#         df['observationdatetime']=pd.to_datetime(df['observationdatetime'], format='%Y-%m-%d %H:%M:%S.%f0')\n",
    "#         df['observationreleasets']=pd.to_datetime(df['observationreleasets'], format='%Y-%m-%d %H:%M:%S.%f0')        \n",
    "        df['observationdatetime']=pd.to_datetime(df['observationdatetime'], format='%d%b%y:%H:%M:%S')\n",
    "        df['observationreleasets']=pd.to_datetime(df['observationreleasets'], format='%d%b%y:%H:%M:%S')\n",
    "        \n",
    "    #load data for sas file\n",
    "    elif input_filename.endswith('.sas7bdat'):\n",
    "        df, _ = pyreadstat.read_sas7bdat(filename_path=input_path+input_filename, \n",
    "                                     row_limit=current_batch_cut_index-last_batch_cut_index,\n",
    "                                     row_offset=last_batch_cut_index)\n",
    "\n",
    "    ##IntegrationPoint02_Close\n",
    "\n",
    "    if len(df) < 1:\n",
    "        return -1\n",
    "    print(f'---------------------- starting batch {batch} ----------------------')\n",
    "    \n",
    "    if input_filename.endswith('.sas7bdat'): \n",
    "        df['observationdatetime'] = pd.to_timedelta(df['observationdatetime'], unit='s') + datetime.datetime(1960,1,1)\n",
    "        df['observationreleasets'] = pd.to_timedelta(df['observationreleasets'], unit='s') + datetime.datetime(1960,1,1)\n",
    "\n",
    "    #decode strings (np objects)\n",
    "    #df1.loc[:, df1.dtypes == np.object] = df1.loc[:, df1.dtypes == np.object].apply(lambda x: x.str.decode('UTF-8'))\n",
    "\n",
    "\n",
    "    df.fillna('', inplace=True)\n",
    "    print('# of records:',len(df))\n",
    "\n",
    "    if output_flag != 1:\n",
    "        df_raw = df.copy(deep = True)\n",
    "\n",
    "    #rename variables\n",
    "    df = df.rename(columns={input_patientid_var:'patientid','fillerordernumber':'fillerordernumberid',\n",
    "                           'observationvalue':'value','observationsubid':'subid'})\n",
    "    #keep key cols\n",
    "    key_cols = ['patientid', 'ordersid', 'fillerordernumberid', \n",
    "                'reportinglaborgname', 'performinglaborgname', 'observationdatetime', \n",
    "                'testrequestcode', 'observationcode', 'observationreleasets', \n",
    "                'observationresultstatus', 'subid', 'value']\n",
    "    df = df[key_cols]\n",
    "\n",
    "    #set exclude_flag based on observationresultstatus = W\n",
    "    df_W = df.loc[df['observationresultstatus'] == 'W', ['ordersid', 'observationcode', 'value']]\n",
    "    df_excl = df[['ordersid', 'observationcode', 'value']].reset_index().merge(df_W, how='inner').set_index('index')\n",
    "    df['exclude_flag'] = 'N'\n",
    "    df.loc[df.index.isin(df_excl.index),['exclude_flag']] = 'Y'\n",
    "    print(df['exclude_flag'].value_counts())\n",
    "\n",
    "    #set exclude_flag based on DO NOT TRANSMIT code\n",
    "    # DNT_text = '<p1:MicroOrganism xmlns:p1=\"http://www.ssha.ca\"><p1:Code>99999999999</p1:Code><p1:Text>Do Not Transmit</p1:Text><p1:CodingSystem>HL79905</p1:CodingSystem></p1:MicroOrganism>'\n",
    "    # df_DNT = df.loc[df['value'] == DNT_text, ['ordersid', 'observationcode','observationreleasets']]\n",
    "    # df_excl2 = df[['ordersid', 'observationcode', 'observationreleasets']].reset_index().merge(df_DNT, how='inner').set_index('index')\n",
    "    # df.loc[df.index.isin(df_excl2.index),['exclude_flag']] = 'Y'\n",
    "    # print(df['exclude_flag'].value_counts())\n",
    "\n",
    "    #%%time\n",
    "    #determine which observations need to be concatenated\n",
    "    group_cols = ['ordersid', 'fillerordernumberid', 'reportinglaborgname', \n",
    "                  'testrequestcode', 'observationcode', 'observationreleasets', 'observationresultstatus']\n",
    "    df_gp_subid = df.reset_index().groupby(group_cols).agg({'index':tuple, 'subid':tuple}).reset_index()\n",
    "    df_gp_subid = df_gp_subid.rename(columns={'index':'original_indexes'})\n",
    "\n",
    "    #only concatenate ones where there are more than two subids, all the subids are numbers and contains 1\n",
    "    df_to_concat = df_gp_subid[df_gp_subid['subid'].apply(lambda x: all([subid.isdigit() for subid in x]) and len(x) > 2 and '1' in x)]\n",
    "    concat_indexes = [i for tup in df_to_concat['original_indexes'] for i in tup]\n",
    "\n",
    "    #concatenate based on subid\n",
    "    df_gp_concat = df[df.index.isin(concat_indexes)].reset_index()\n",
    "    df_gp_concat['subid'] = df_gp_concat['subid'].apply(int)\n",
    "    df_gp_concat = df_gp_concat.sort_values(by = group_cols+['subid']).groupby(group_cols)\n",
    "    df_gp_concat = df_gp_concat.agg({'index': tuple,\n",
    "                       'value': lambda x: ' '.join(map(str, x))}).reset_index()\n",
    "\n",
    "    #add on records that were not concatenated\n",
    "    df_gp = df.loc[~df.index.isin(concat_indexes), group_cols+['value']].reset_index()\n",
    "    df_gp['index'] = df_gp['index'].apply(lambda x: (x,))\n",
    "    df_gp = pd.concat([df_gp_concat, df_gp], sort=False).rename(columns={'index':'original_indexes'})\n",
    "\n",
    "    #narrow down columns of df\n",
    "    df_cols = ['patientid','ordersid','fillerordernumberid','observationdatetime','testrequestcode',\n",
    "               'observationcode','observationreleasets', 'observationresultstatus','exclude_flag']\n",
    "    df = df[df_cols]\n",
    "\n",
    "    print('# of TEST RESULTS:', len(df_gp))\n",
    "\n",
    "    #cleanup\n",
    "    del df_W\n",
    "    del df_excl\n",
    "    del df_gp_subid\n",
    "    del df_to_concat\n",
    "    del concat_indexes\n",
    "    del df_gp_concat\n",
    "\n",
    "    #clean punctuation, xml field, numbers, other text\n",
    "    puncs = [';', ':', ',', '.', '-', '_', '/', '(', ')', '[', ']', '{', '}', '<', '>', '*', '#', '?', '.', '+', \n",
    "            'br\\\\', '\\\\br', '\\\\e\\\\', '\\\\f\\\\', '\\\\t\\\\', '\\\\r\\\\', '\\\\', \"'\", '\"', '=']\n",
    "    terms_to_space = ['detected', 'by', 'positive', 'parainfluenza', 'accession']\n",
    "    nums_following = ['date', 'telephone', 'tel', 'phone', 'received', 'collected',  \n",
    "                     'result', 'on', 'at', '@', 'approved', 'final', 'time', 'number']\n",
    "    strings_to_replace = {'non detected':'not detected','nt detected':'not detected' ,'npot detected':'not detected', \n",
    "                          'nor detected':'not detected', 'mot detected':'not detected', \n",
    "                          'n0t detected':'not detected', 'nit detected':'not detected','agenot detected':'not detected',\n",
    "                          'covid 19 virus not interpretation detected':'covid 19 virus interpretation not detected',\n",
    "                          'presumptive interpretation':'interpretation presumptive',\n",
    "                          'preliminary interpretation':'interpretation preliminary',\n",
    "                          'covid 19 not detected and covid 19 detected':'covid 19 detected and covid 19 not detected',\n",
    "                          'virusnot':'virus not', 'prevuous':'previous'}\n",
    "    date_id_patterns = [r'\\d{2,4} \\d{2} \\d{2,4} ', r'\\d{4} \\d{2} ', r'\\d{4}h ', \n",
    "                       r' \\d{0,2}[a-z]{0,2}\\d{5,}[a-z]{0,1}', r' [a-z]{0,2}\\d{1,3}[a-z]{1,3}\\d{4,}[a-z]{0,1}',\n",
    "                       r' \\d{2}[a-z]{1}\\d{3}[a-z]{2}\\d{4}', r' [a-z]{4,}\\d{7,}']\n",
    "\n",
    "    def clean(value):\n",
    "        cleaned = value.lower()\n",
    "\n",
    "        #clean xml field, only keep text field surrounded with 'p1 text'\n",
    "        pattern = r'(<p1:microorganism xmlns)(.+)(<p1:text>.+</p1:text>)(.+)(</p1:microorganism>)'\n",
    "        while re.search(pattern, cleaned):\n",
    "            cleaned = re.sub(pattern, r'\\g<3>', cleaned)\n",
    "\n",
    "        #surround terms with spaces (some terms found stuck together)\n",
    "        for t in terms_to_space:\n",
    "            cleaned = cleaned.replace(t, ' ' + t + ' ')\n",
    "\n",
    "        #replace punctuation with space\n",
    "        for punc in puncs:\n",
    "            cleaned = cleaned.replace(punc, ' ')\n",
    "\n",
    "        #remove consecutive spaces\n",
    "        while '  ' in cleaned:\n",
    "            cleaned = cleaned.replace('  ', ' ')\n",
    "\n",
    "        cleaned = cleaned.strip()     \n",
    "\n",
    "        #remove numbers after certain terms\n",
    "        for term in nums_following:\n",
    "            pattern = term + r' \\d{1,4}'\n",
    "\n",
    "            while re.search(pattern, cleaned):\n",
    "                cleaned = re.sub(pattern, term, cleaned)\n",
    "\n",
    "        #remove more dates and ids\n",
    "        for pattern in date_id_patterns:\n",
    "            while re.search(pattern, cleaned):\n",
    "                cleaned = re.sub(pattern, '', cleaned)\n",
    "\n",
    "        #remove numbers at the end\n",
    "        while len(cleaned) > 0 and (cleaned[-1].isdigit() or cleaned[-1] == ' '):\n",
    "            cleaned = cleaned[:-1]\n",
    "\n",
    "        #remove \"no\" at the end\n",
    "        while cleaned.endswith(' no') or cleaned == 'no':\n",
    "            cleaned = cleaned[:-3]\n",
    "\n",
    "        #fix certain strings\n",
    "        for k, v in strings_to_replace.items():\n",
    "            cleaned = cleaned.replace(k, v)\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    #tokenize values using nltk\n",
    "    def tokenize(value):\n",
    "        tokenized = nltk.word_tokenize(value)\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    #assign labels for useful tokens based on some dictionaries and exclusions\n",
    "    easy_virus_dict = {'v_adenovirus':['aden'], 'v_bocavirus':['boca', 'bocca'], 'v_coronavirus':['coro', 'cora'],\n",
    "                       'v_entero_rhino':['enterol', 'enterov', 'entervir', 'rhino', 'rhini'], 'v_hmv':['metap']}\n",
    "    hard_virus_dict = {'v_rsv':['rsv'], 'v_flu':['nflu', 'flue','flua','flub'], 'v_para':['parai', 'pata', 'parta'],\n",
    "                       'v_covid':['cov', 'sars', 'orf1', 'orfl', 'or1lab']} #fluvid \n",
    "    indirect_matches_dict = {'r_pos': ['posi','pos1','covpos'], \n",
    "                             'r_neg': ['neg', 'naeg', 'neag'],  \n",
    "                             'r_ind': ['indeter', 'eterminate', 'inconclu', 'inderter',\n",
    "                                       'equivocal', 'unresolved'],\n",
    "                             'r_can': ['cancel', 'incorrect', 'duplicate', 'mislabel','unlabel',\n",
    "                                       'recollect', 'mistaken', 'wrong', 'redirect'],\n",
    "                             'r_rej': ['reject', 'inval', 'leak', 'unable', 'insuffic', \n",
    "                                       'spill', 'inapprop', 'nsq', 'poor', 'uninterpret'],\n",
    "                             'presumptive': ['presump', 'prelim', 'possi'], \n",
    "                             'retest': ['retest']} #'sent', 'send', 'forwarded'\n",
    "    direct_matches_dict = {'r_pos': ['detected', 'pos', 'deteced', 'postive', 'organism','isolated'],\n",
    "                           'r_neg': ['no', 'not'],\n",
    "                           'r_ind': ['ind'],\n",
    "                           'r_pen': ['pending', 'progress', 'follow', 'ordered', 'reordered','reorder'],\n",
    "                           'presumptive': ['single', 'possible', 'probable'],\n",
    "                           'xml': ['p1'], \n",
    "                           'reset': ['deleted','anesthesiologist'],\n",
    "                           'stop': ['specific', 'required', 'error', 'copy', 'see', 'laboratory',\n",
    "                                    'note', 'stability', 'changed', 'recollect', 'moh', 'if', 'before'],\n",
    "                           'final': ['interpretation', 'interpetation', 'interp', 'pretation', 'interpretive',\n",
    "                                     'final', 'overall', 'corrected', 'proved', 'correct','current'],\n",
    "                           'skip': ['reason', 'identify', 'confirmation'],\n",
    "                           'end': ['mutation','voc','vocs','variant','variants','serology'],\n",
    "                           'connecting': ['screen', 'presence', 'as', 'real',\n",
    "                                          'is', 'of', 'in', '1', '2', '3', '4', 'a', 'b', 'c',\n",
    "                                          '229e', 'nl63', 'hku1', 'oc43', '19', '2019', 'low',\n",
    "                                          'biosafety', 'hazard', 'has', 'been', 'for', 'changed', 'identified', \n",
    "                                          'result', 'other', 'testing', 'using', 'to', 'from', 'tested',\n",
    "                                          'phl', 'phol', 'phlo', 'new', 'request', 'lab', 'will',\n",
    "                                          'panel', 'seasonal', 'human', 'report', 'said', 'updated', 'dob',\n",
    "                                          'requisition', 'form','label'\n",
    "                                          ]}\n",
    "    test_type_dict = {'t_oth': ['eia', 'rapid', 'immunoassay', 'ict', 'immunochromatographic', 'antigen'], \n",
    "                      't_pcr': ['multiplex', 'naat', 'nat', 'pcr', 'rrt', 'rna', 'gen', \n",
    "                                'reverse', 'polymerase', 'chain', 'simplexa'],\n",
    "                      't_gene': ['gene', 'targets', 'tagets', 'target']}\n",
    "\n",
    "    def assign_labels(tokenized):\n",
    "        tokenized_length = len(tokenized)\n",
    "        useful = [None]*tokenized_length #store same list length of tokens and update each accordingly\n",
    "\n",
    "        for counter, token in enumerate(tokenized):\n",
    "\n",
    "            #skip if already assigned\n",
    "            if useful[counter]:\n",
    "                continue\n",
    "\n",
    "            ###easy viruses dictionary (non-exact matching)\n",
    "            for virus, patterns in easy_virus_dict.items():\n",
    "                if any([pattern in token for pattern in patterns]):\n",
    "                    useful[counter] = virus\n",
    "                    break\n",
    "\n",
    "            #extra rhino/entero rule (exact matching)\n",
    "            if token in ('rhino', 'entero'):\n",
    "                useful[counter] = 'v_entero_rhino'\n",
    "\n",
    "            ###hard viruses dictionary (non-exact matching)\n",
    "\n",
    "            #COVID19 \n",
    "            #e/envelope/n/nucleocapsid/s/spike gene\n",
    "            elif token in ('e', 'envelope', 'n', 'nucleocapsid', 's', 'spike') and (tokenized_length > counter+1)\\\n",
    "            and tokenized[counter+1] == 'gene':\n",
    "                useful[counter:counter+2] = ['v_covid', 't_gene']\n",
    "\n",
    "            #rdrp gene\n",
    "            elif token == 'rdrp' and (tokenized_length > counter+1)\\\n",
    "            and tokenized[counter+1] == 'gene' and 'v_coronavirus' not in useful[:counter]:\n",
    "                useful[counter:counter+2] = ['v_covid', 't_gene']\n",
    "\n",
    "            #orf1ab\n",
    "            elif any([pattern in token for pattern in ['orf1','orfl','or1lab']]) and (tokenized_length > counter+1)\\\n",
    "            and tokenized[counter+1] == 'gene' and 'mers' not in tokenized[counter-3:counter]:\n",
    "                useful[counter:counter+2] = ['v_covid', 't_gene']\n",
    "\n",
    "            elif any([pattern in token for pattern in hard_virus_dict['v_covid']])\\\n",
    "            and not any([pattern in token for pattern in ('ecov', 'cove', 'covpos')])\\\n",
    "            and not any([word==pattern for word in tokenized[counter-3:counter] \n",
    "                         for pattern in ('mers', 'caesarean', 'xpress')])\\\n",
    "            and not tokenized[counter-1] == 'non':\n",
    "                useful[counter] = 'v_covid'\n",
    "\n",
    "            #extra rule for seasonal coronavirus, if preceded by novel or followed by 19/disease/cov/sars/2\n",
    "            elif any([pattern in token for pattern in easy_virus_dict['v_coronavirus']]):\n",
    "                if 'nove' in tokenized[counter-1] or tokenized[counter-1] == 'nivel':\n",
    "                    useful[counter-1:counter+1] = ['connecting', 'v_covid']\n",
    "\n",
    "                covid_extra = [] #extra terms\n",
    "                look_forward = 3 #how many terms to look forward for\n",
    "                max_forward = min(counter+look_forward, tokenized_length-1) #limit if record is too short\n",
    "                covid_extra = [(tokenized[covid_pos], covid_pos) for covid_pos in range(counter+1, max_forward+1)\\\n",
    "                           if any([pattern in tokenized[covid_pos] for pattern in ('19', 'disea', 'cov', 'sars')]\\\n",
    "                                  +[tokenized[covid_pos] == '2'])]\n",
    "\n",
    "                #assign range of relevant tokens as virus change and 'non' not in tokenized[counter+1:max_forward+1]\n",
    "                if len(covid_extra) > 0 and 'non' not in tokenized[counter+1:max_forward+1]:\n",
    "                    last_pos = max([x[1] for x in covid_extra])\n",
    "                    useful[counter:last_pos+1] = ['v_covid']+['connecting']*(last_pos-counter)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            #PARA\n",
    "            elif any([pattern in token for pattern in hard_virus_dict['v_para']]+[token == 'para'])\\\n",
    "            and tokenized[counter-1] != 'haemophilus':\n",
    "                para_extra = []\n",
    "                look_forward = 5\n",
    "                max_forward = min(counter+look_forward, tokenized_length-1)\n",
    "                para_extra = [(tokenized[para_pos], para_pos) for para_pos in range(counter+1, max_forward+1)\\\n",
    "                                  if tokenized[para_pos] in ('1','2','3','4')]\n",
    "\n",
    "                if len(para_extra) > 0:\n",
    "                    last_pos = max([x[1] for x in para_extra])\n",
    "                    para_nums = [x[0] for x in para_extra]\n",
    "                    useful[counter:last_pos+1] = ['v_para_' + '_'.join(para_nums)]+['connecting']*(last_pos-counter)\n",
    "                else:\n",
    "                    useful[counter] = 'v_para'\n",
    "\n",
    "            #FLU\n",
    "            elif any([pattern in token for pattern in hard_virus_dict['v_flu']]+[token in ('flu', 'inf')])\\\n",
    "            and tokenized[counter-1] != 'haemophilus' and tokenized[counter] != 'fluab':\n",
    "                flu_extra = []\n",
    "                look_forward = 4\n",
    "                max_forward = min(counter+look_forward, tokenized_length-1)\n",
    "\n",
    "                for flu_pos in range(counter+1, max_forward+1):\n",
    "                    if tokenized[flu_pos] in ('a','b') or 'h1' in tokenized[flu_pos] or 'h3' in tokenized[flu_pos]:\n",
    "                        flu_extra.append((tokenized[flu_pos], flu_pos))\n",
    "                    elif 'flu' in tokenized[flu_pos]: #to deal with influenza a influenza b\n",
    "                        break\n",
    "\n",
    "                if len(flu_extra) > 0:\n",
    "                    last_pos = max([x[1] for x in flu_extra])\n",
    "                    flu_types = [x[0] for x in flu_extra]\n",
    "                    if 'a' in flu_types and 'b' in flu_types:\n",
    "                        useful[counter:last_pos+1] = ['v_flu_a_b']+['connecting']*(last_pos-counter)\n",
    "                    elif 'b' in flu_types:\n",
    "                        useful[counter:last_pos+1] = ['v_flu_b']+['connecting']*(last_pos-counter)\n",
    "                    elif any(['h1' in f for f in flu_types]) and any(['h3' in f for f in flu_types]):\n",
    "                        useful[counter:last_pos+1] = ['v_flu_a_h1_h3']+['connecting']*(last_pos-counter)\n",
    "                    elif any(['h1' in f for f in flu_types]):\n",
    "                        useful[counter:last_pos+1] = ['v_flu_a_h1']+['connecting']*(last_pos-counter)\n",
    "                    elif any(['h3' in f for f in flu_types]):\n",
    "                        useful[counter:last_pos+1] = ['v_flu_a_h3']+['connecting']*(last_pos-counter)\n",
    "                    elif 'a' in flu_types:\n",
    "                        useful[counter:last_pos+1] = ['v_flu_a']+['connecting']*(last_pos-counter)                                                                  \n",
    "                elif token.endswith('aa'):\n",
    "                    useful[counter] = 'v_flu_a'\n",
    "                elif token.endswith('ab'):\n",
    "                    useful[counter] = 'v_flu_b'\n",
    "                else:\n",
    "                    useful[counter] = 'v_flu'\n",
    "\n",
    "            #RSV\n",
    "            elif any([pattern in token for pattern in hard_virus_dict['v_rsv']]):\n",
    "                rsv_extra = []\n",
    "                look_forward = 2\n",
    "                max_forward = min(counter+look_forward, tokenized_length-1) \n",
    "                rsv_extra = [(tokenized[rsv_pos], rsv_pos) for rsv_pos in range(counter+1, max_forward+1)\\\n",
    "                           if tokenized[rsv_pos] in ('a','b')]\n",
    "\n",
    "                if len(rsv_extra) > 0:\n",
    "                    last_pos = max([x[1] for x in rsv_extra])\n",
    "                    rsv_types = [x[0] for x in rsv_extra]\n",
    "                    if 'a' in rsv_types and 'b' in rsv_types:\n",
    "                        useful[counter:last_pos+1] = ['v_rsv_a_b']+['connecting']*(last_pos-counter)\n",
    "                    elif 'a' in rsv_types:\n",
    "                        useful[counter:last_pos+1] = ['v_rsv_a']+['connecting']*(last_pos-counter)\n",
    "                    elif 'b' in rsv_types:\n",
    "                        useful[counter:last_pos+1] = ['v_rsv_b']+['connecting']*(last_pos-counter)\n",
    "                else:\n",
    "                    useful[counter] = 'v_rsv'\n",
    "\n",
    "            elif (tokenized_length > counter+2) and ((token.startswith('resp')\\\n",
    "            and tokenized[counter+1].startswith('syn') and tokenized[counter+2].startswith('vi'))\\\n",
    "            or (token == 'r' and tokenized[counter+1] == 's' and tokenized[counter+2] == 'v')):\n",
    "                rsv_extra = []\n",
    "                look_forward = 4\n",
    "                max_forward = min(counter+look_forward, tokenized_length-1) \n",
    "                rsv_extra = [(tokenized[rsv_pos], rsv_pos) for rsv_pos in range(counter+3, max_forward+1)\\\n",
    "                           if tokenized[rsv_pos] in ('a','b')]\n",
    "\n",
    "                if len(rsv_extra) > 0:\n",
    "                    last_pos = max([x[1] for x in rsv_extra])\n",
    "                    rsv_types = [x[0] for x in rsv_extra]\n",
    "                    if 'a' in rsv_types and 'b' in rsv_types:\n",
    "                        useful[counter:last_pos+1] = ['v_rsv_a_b']+['connecting']*(last_pos-counter)\n",
    "                    elif 'a' in rsv_types:\n",
    "                        useful[counter:last_pos+1] = ['v_rsv_a']+['connecting']*(last_pos-counter)\n",
    "                    elif 'b' in rsv_types:\n",
    "                        useful[counter:last_pos+1] = ['v_rsv_b']+['connecting']*(last_pos-counter)\n",
    "                else:\n",
    "                    useful[counter:counter+3] = ['v_rsv', 'connecting', 'connecting']\n",
    "\n",
    "            #UNKNOWN VIRUS\n",
    "            elif (token.startswith('vir') or token.startswith('viu')):\n",
    "                #extra rule for virus culture\n",
    "                if (tokenized_length > counter+2) and tokenized[counter+1].startswith('cult')\\\n",
    "                and 'request' in tokenized[counter+2]:\n",
    "                    useful[counter:counter+3] = ['connecting']*3\n",
    "                elif (tokenized_length > counter+1) and tokenized[counter+1].startswith('cult'):\n",
    "                    useful[counter:counter+2] = ['t_oth']*2\n",
    "                else:\n",
    "                    useful[counter] = 'v_unk'\n",
    "\n",
    "            #extra terms to treat as an \"unknown virus\" for purpose of algorithm\n",
    "            elif token in ('by','further','specimen','specimens','test','sample','considered'):\n",
    "                useful[counter] = 'v_unk'\n",
    "\n",
    "        # loop over the record again\n",
    "        for counter, token in enumerate(tokenized):\n",
    "\n",
    "            #skip if already assigned\n",
    "            if useful[counter]:\n",
    "                continue\n",
    "\n",
    "            #culture tests  \n",
    "            if token.startswith('cult') and not ((tokenized_length > counter+1) and 'request' in tokenized[counter+1]):\n",
    "                useful[counter] = 't_oth'\n",
    "\n",
    "            #additional \"direct\" tests\n",
    "            elif token == 'direct' and (tokenized_length > counter+1):\n",
    "                if tokenized[counter+1] in ('kit', 'enzyme', 'test', 'testing', 'eia', 'antigen', 'ict'):\n",
    "                    useful[counter:counter+2] = ['t_oth']*2\n",
    "                elif tokenized[counter+1] in ('influenza',):\n",
    "                    useful[counter] = 't_oth'\n",
    "\n",
    "            #condition for mention of pos/neg\n",
    "            elif token in ('negative','neg','positive','pos','detected','organism')\\\n",
    "            and (tokenized_length > counter+1)\\\n",
    "            and ((tokenized[counter-1] in ('a','original','or','level','of','the','tested','was','false','had','antibodies','antibody') \n",
    "                  and tokenized[counter+1] in ('test','result','covid','new','at','note','sars')) \n",
    "                 or tokenized[counter+1] in ('or','swab','to','contact','workers','retest','results',\n",
    "                                             'son','person','patients','travel','individual','admission',\n",
    "                                             'roommate','rapid','coworker','co','cultures')):\n",
    "                useful[counter-1:counter+2] = [None]*3\n",
    "            elif token in ('negative','neg','positive','pos','detected','organism','posivtive')\\\n",
    "            and (tokenized_length > 1)\\\n",
    "            and (tokenized[counter-2] in ('previous','previously','contact','worker','depot','targets',\n",
    "                                          'being','unless','patient','law','due','exposure','needs','if',\n",
    "                                          'swab','who','psw','known','must','mother')\n",
    "                 or tokenized[counter-1] in ('previous','previously','known','unit','first','second',\n",
    "                                             'needs','need','requires','considered','swab','if',\n",
    "                                             'depot','employee','gram','cx','member','coworker','shows',\n",
    "                                             'father','contact','both','and','confirm','despite','provide',\n",
    "                                             'mom','verify','particles')):\n",
    "                useful[counter-1:counter+1] = [None]*2\n",
    "            elif token in ('negative','neg','positive','pos','detected','organism')\\\n",
    "            and (tokenized_length > 3)\\\n",
    "            and (tokenized[counter-3] in ('mom','him','father','which','sister','partner','contact','who')\n",
    "                or tokenized[counter-4] in ('who',)):\n",
    "                useful[counter-2:counter+1] = [None]*3 \n",
    "\n",
    "            #condition for word before no\n",
    "            elif token == 'no' and (tokenized[counter-1] in ('by','lab','specimen','accession',\n",
    "                                                             'sample','order','please','phl',\n",
    "                                                             'with','bipap','pain','ord')\n",
    "                                    or any([pattern in tokenized[counter-1] for pattern in ('out','break','inv')]))\\\n",
    "            and tokenized[counter+1:counter+2] != ['virus'] and counter > 0:\n",
    "                useful[counter-1:counter+1] = [None]*2\n",
    "\n",
    "            #condition for word after no (skip)\n",
    "            elif token == 'no' and (tokenized_length > counter+1)\\\n",
    "            and tokenized[counter+1] in ('fever','answer','longer','patient','second','grh','afb',\n",
    "                                         'exposure','appetite','outbreak','symptoms','symmptoms',\n",
    "                                         'show','2nd','transfer','requisition','evidence','dob',\n",
    "                                         'response','name','unexplained','lts','hc','hcn','paperwork',\n",
    "                                         'further','pick','time','call','other','viral', 'need', 'salmonella', \n",
    "                                         'date', 'submission', 'health','subsitute','cough','two','identifying','match',\n",
    "                                         'growth','birthday'):\n",
    "                useful[counter] = 'skip'\n",
    "\n",
    "            #condition for word after no (cancel)\n",
    "            elif token == 'no' and (tokenized_length > counter+1)\\\n",
    "            and tokenized[counter+1] in ('specimen','reportable','done','gene','result',\n",
    "                                         'media','liquid','sample','swab','nasopharyngeal','record','fluid',\n",
    "                                         'patient','second','results','testing','eluate','option','chose',\n",
    "                                         'speicmen','label','validated','culture','volume','saliva','requisiton',\n",
    "                                         'naso','confirmatory','dry'): \n",
    "                useful[counter] = 'r_can'\n",
    "\n",
    "            #condition for due to\n",
    "            elif tokenized[counter:counter+2] in [['due','to'],['for','result']] and 'new' not in tokenized[counter+2:counter+4]:\n",
    "                useful[counter:counter+2] = ['stop']*2\n",
    "\n",
    "            #condition for word after not (skip)\n",
    "            elif token == 'not' and (tokenized_length > counter+1)\\\n",
    "            and tokenized[counter+1] in ('test','been','suspicious','validated','valildated','the','admitted',\n",
    "                                         'preclude','intended','likely','given','coming','verified','ha','covid',\n",
    "                                         'for','signed','on','recommended','retrieve','corssing','swabbed',\n",
    "                                         'beingberecollected','at','eligible','vaccinated','put', 'leaking', 'per',\n",
    "                                         'approved','c','nose','only','informed'):\n",
    "                useful[counter:counter+2] = ['skip']*2\n",
    "\n",
    "\n",
    "            #condition for word after not (cancel)\n",
    "            elif token == 'not' and (tokenized_length > counter+1) and \\\n",
    "            tokenized[counter+1] in ('tested','tessted','perform','performed','process','processed', \n",
    "                                     'transmit','suitable','done','doen','be','reported','received', \n",
    "                                     'match','needed','labelled','available','symptomatic','forwared',\n",
    "                                     'met','specified','indicated','returned','sufficient',\n",
    "                                     'valid','required','able','needed','contain','ordered','recieved',\n",
    "                                     'labeled','a','provided','appropriate','sent','send','remove',\n",
    "                                     'report','rapid','found','applicable','rec','used','order',\n",
    "                                     'matched','labled','proccessed','accepted','receivd','completed',\n",
    "                                     'recollect','preformed','appearing','in','collected','obtained',\n",
    "                                     'acceptable','capped','requested','enough','an','clearly','midturbinate',\n",
    "                                     'refrigerated','refrigirated'):\n",
    "                useful[counter:counter+2] = ['r_can']*2\n",
    "\n",
    "            #condition for word before not\n",
    "            elif token == 'not' and tokenized[counter-1] in ('does','did','please','done','over','swab','but','do','can','or'): #can?\n",
    "                useful[counter-1:counter+1] = ['skip']*2\n",
    "\n",
    "            #condition for errors\n",
    "            elif tokenized[counter:counter+3] in (['ordered','in','error'], ['positive','in','error'],\n",
    "                                                 ['no','covid','result'], ['pos','in','error'],\n",
    "                                                 ['sent','under','incorrect'], ['no','covid','order'],\n",
    "                                                 ['s','already','positive']): \n",
    "                useful[counter:counter+3] = ['r_can']*3\n",
    "            elif tokenized[counter:counter+3] in (['added','in','error'], ['the','wrong','patient'],['reported','in','error']):\n",
    "                useful[counter:counter+3] = ['reset']*3\n",
    "            elif tokenized[counter:counter+2] in (['processing','error'], ['in','error'], ['same','test'],\n",
    "                                                  ['labelling','error']):\n",
    "                useful[counter:counter+2] = ['r_can']*2\n",
    "            elif tokenized[counter:counter+4] in (['report','please','disregard','covid'],\n",
    "                                                  ['patient','please','disregard','results'],\n",
    "                                                  ['report','please','disregard','report'],\n",
    "                                                  ['individual', 'target', 'result', 'interpretation'],\n",
    "                                                  ['individual', 'target', 'results', 'interpretation'],\n",
    "                                                  ['requesting', 'negative', 'covid' ,'swab'],\n",
    "                                                  ['detected', 'cycle', 'threshold' ,'35'],\n",
    "                                                  ['does','not','belong','to'],\n",
    "                                                  ['outbreak', 'no', 'bdpoc', 'valid'],\n",
    "                                                  ['father', 'tested', 'positive', 'for'],\n",
    "                                                  ['multiplex', 'covid', 'flu', 'rspending'],\n",
    "                                                  ['report', 'positive', 'family','member'],\n",
    "                                                  ['organism', 'quantitation', 'of', 'growth'],\n",
    "                                                  ['please', 'disregard', 'covid', '19'],\n",
    "                                                  ['was', 'chosen', 'in', 'error']):\n",
    "                useful[counter:counter+4] = ['end']*4\n",
    "            elif tokenized[counter:counter+4] in (['please','remove','previous','copies'],):\n",
    "                useful[counter:counter+4] = ['v_covid','r_can','r_can','r_can']\n",
    "\n",
    "            #condition for target rna and disregard\n",
    "            elif tokenized[counter:counter+2] in (['target','rna'],['patient','disregard'] ): \n",
    "                useful[counter:counter+2] = ['end']*2\n",
    "\n",
    "            #condition for mother positive \n",
    "            elif token == 'mother' and (tokenized_length > counter+1)\\\n",
    "            and tokenized[counter+1] in ('positive'):\n",
    "                useful[counter:counter+2] = ['skip']*2\n",
    "\n",
    "\n",
    "            #condition for previous\n",
    "            elif 'previous' in token and ('reported' in tokenized[counter+1:counter+3] or\n",
    "                                          'specimen' in tokenized[counter+1:counter+2] or\n",
    "                                          (tokenized[counter+1:counter+3] == ['report','covid'] and\n",
    "                                               tokenized[counter-1] == 'the') or\n",
    "                                          tokenized[counter+1:counter+3] in (['report','of'],\n",
    "                                                                             ['reports','of'],\n",
    "                                                                             ['reportof','covid'],\n",
    "                                                                             ['result','of'],\n",
    "                                                                             ['covid','19'],\n",
    "                                                                             ['entered','covid'],\n",
    "                                                                             ['report','as'],\n",
    "                                                                             ['result','was'],\n",
    "                                                                             ['report','that'])):\n",
    "                useful[counter:counter+2] = ['end']*2\n",
    "\n",
    "            #unable and indeterminate\n",
    "            elif tokenized[counter:counter+5] in (['unable','to','be','completed','indeterminate'],\n",
    "                                                 ['please','disregard','previous','indeterminate','result'],\n",
    "                                                 ['interpretation','possible','low','viral','load'],\n",
    "                                                 ['all','positive','and','indeterminate','results'],\n",
    "                                                 ['19','positive','e','e','ab'],\n",
    "                                                 ['repeat', 'gx', 'e', 'neg', 'n'],\n",
    "                                                 ['for', 'clinical', 'use', 'indeterminate', 'for'],\n",
    "                                                 ['indeterminate', 'cycle', 'threshold','99', 'specimen'],\n",
    "                                                 ['in', 'low', 'has', 'positive','covid'],\n",
    "                                                ['rejection','interface','error','specimen','not'],\n",
    "                                                  ['husband', 'swabbed', 'positive', 'for', 'covid'],\n",
    "                                                  ['born', 'to', 'covid', 'positive', 'mother'],\n",
    "                                                  ['newborn', 'to', 'covid', 'positive', 'mother'],\n",
    "                                                  ['to', 'covid', '19', 'positive','mother'],\n",
    "                                                  ['roommate', 'was', 'covid', 'positive', 'day'],\n",
    "                                                  ['delivered', 'to', 'covid', 'positive', 'mother'],\n",
    "                                                  ['exposed', 'to', 'a', 'positive', 'case'],\n",
    "                                                  ['exposed', 'to', 'covid', 'positive', 'case'],\n",
    "                                                  ['need', 'confirmation', 'of', 'negatvie', 'swab'],\n",
    "                                                  ['icu', 'admission', 'no', 'covid', 'swab'],\n",
    "                                                  ['positive', 'patient', 'sample', 'submitted', 'for']\n",
    "                                                ):\n",
    "\n",
    "                useful[counter:counter+4] = ['connecting']*4\n",
    "\n",
    "            #upon further investigation\n",
    "            elif tokenized[counter:counter+3] in (['upon','further','investigation'],):\n",
    "\n",
    "                useful[counter-2:counter] = ['reset']*2\n",
    "\n",
    "            #exclude antibody and reordered\n",
    "            elif tokenized[counter:counter+8] in (['negative','for','anti','sars','cov','2','igg','antibodies'],\n",
    "                                                  ['negative','for','anti','sars','cov','2','iga','antibodies'],\n",
    "                                                 ['positive', 'for', 'anti', 'sars', 'cov' ,'2', 'igg', 'antibodies'],\n",
    "                                                 ['positive', 'for', 'anti', 'sars', 'cov' ,'2', 'iga', 'antibodies'],\n",
    "                                                 ['campylobacter', 'yersinia', 'or', 'e', 'coli', '0157', 'h7', 'isolated'],\n",
    "                                                 ['client', 'disclosed', 'chosen', 'not', 'to', 'vaccinat', 'for', 'covid'],\n",
    "                                                 ['forwarded', 'to', 'public', 'health', 'for', 'covid', 'flu', 'and'],\n",
    "                                                 ['label','on','specimen','not','matching','please','re','collect'],\n",
    "                                                 ['cepheid', 'multiplex', 'covid', 'flu', 'rsfinal', 'test', 'not', 'performed'],\n",
    "                                                 ['and', 'covidscr', 'are', 'ordered', 'on', 'the', 'same', 'swab'],\n",
    "                                                 ['neg', 'pos', 'rpt','pos', 'further', 'report', 'to', 'follow'],\n",
    "                                                 ['disregard', 'previous', 'result', 'stating', 'covid', '19', 'virus', 'detected'],\n",
    "                                                  ['the', 'university', 'health', 'covid', '19', 'virus', 'not', 'detected'],\n",
    "                                                  ['detected', 'sars', 'cov', '2', 'n', 'gene', 'low', 'positive'],\n",
    "                                                  ['to', 'covid', '19', 'positive', 'case', 'daughter', 'visited', 'yesterday'],\n",
    "                                                  ['of', 'dry', 'coughing', 'and', 'no', 'covid', 'test', 'in'],\n",
    "                                                  ['admitted', 'to', 'nicu', 'exposed', 'to', 'covid', 'positive', 'mother']\n",
    "                                                 ):\n",
    "\n",
    "                useful[counter:counter+8] = ['None']*8\n",
    "\n",
    "            #assign pending\n",
    "            elif tokenized_length==3 and tokenized[counter:counter+3] == ['see','scanned','result']: \n",
    "                useful[counter:counter+3] = ['r_pen']*3 \n",
    "\n",
    "            #exclude accidently picked-up negatives\n",
    "            elif 'inneg' in token or 'lineag' in token:\n",
    "                useful[counter] = 'connecting'\n",
    "\n",
    "            else:\n",
    "                #indirect_matches dictionary\n",
    "                for term, patterns in indirect_matches_dict.items():\n",
    "                    if any([pattern in token for pattern in patterns]):\n",
    "                        useful[counter] = term\n",
    "                        break\n",
    "\n",
    "                #direct_matches dictionary\n",
    "                for term, patterns in direct_matches_dict.items():\n",
    "                    if any([pattern == token for pattern in patterns]):\n",
    "                        useful[counter] = term\n",
    "                        break\n",
    "\n",
    "                #test_type dictionary\n",
    "                for test, patterns in test_type_dict.items():\n",
    "                    if any([pattern == token for pattern in patterns]):\n",
    "                        useful[counter] = test\n",
    "                        break\n",
    "\n",
    "        return useful\n",
    "\n",
    "    #interpret text to get initial results\n",
    "    def interpret(useful):\n",
    "\n",
    "        def save(b):\n",
    "            #presumptive modifier\n",
    "            if b[1] == 'r_pos' and modifier[1]:\n",
    "                b[1] = 'r_pre'\n",
    "\n",
    "            #end modifier (skips a save)\n",
    "            if not modifier[2] or modifier[0]:\n",
    "                output.append([b[0] if b[0] else 'v_unk', \n",
    "                               b[1] if b[1] else 'r_neg', \n",
    "                               b[2] if b[2] else 't_unk', \n",
    "                               modifier[0]]) #final modifier\n",
    "\n",
    "            b[0] = None\n",
    "            b[1] = None\n",
    "            modifier[0:4] = [False, False, False, False]\n",
    "            return\n",
    "\n",
    "        sentence = useful[:]\n",
    "        output = []\n",
    "\n",
    "        #bundle for current virus/result/test\n",
    "        #0 = virus, 1 = result, 2 = test\n",
    "        bundle = [None, None, None]\n",
    "\n",
    "        #modifiers\n",
    "        #0 = final, 1 = presumptive, 2 = end, 3 = skip\n",
    "        modifier = [False, False, False, False]\n",
    "\n",
    "        none_counter = 0 #counter for hitting consecutive irrelevant words\n",
    "        virus_counter = 0 #counter for different viruses in same segment\n",
    "\n",
    "        #xml field processing\n",
    "        xml_pos = [i for i, x in enumerate(sentence) if x == 'xml']\n",
    "        num = len(xml_pos)//2\n",
    "        for i in range(num):\n",
    "            xml_start_pos = xml_pos[i*2]\n",
    "            xml_end_pos = xml_pos[i*2+1]\n",
    "            for j in range(xml_start_pos, xml_end_pos + 1):\n",
    "                if sentence[j] and sentence[j].startswith('v_') and sentence[j] != 'v_unk':\n",
    "                    bundle[0] = sentence[j]\n",
    "                    bundle[1] = 'r_pos'\n",
    "                    save(bundle)\n",
    "\n",
    "        #add result to output if result in first 3 words\n",
    "        if len(sentence) > 3 and not any(['v_' in s for s in sentence[0:3] if s]):\n",
    "            for s in sentence[0:3]:\n",
    "                if s and 'r_' in s:\n",
    "                    output.append(['v_unk', s, 't_unk', False])\n",
    "                    break\n",
    "                elif s and s == 'connecting':\n",
    "                    pass\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        #if there is mention of retest but no final result flag, assign final flag to start\n",
    "        if ('retest' in sentence or (len(sentence) > 0 and sentence[0] == 'r_ind')) and 'final' not in sentence :\n",
    "            modifier[0] = True\n",
    "\n",
    "        #loop on words in sentence\n",
    "        for word in sentence:\n",
    "\n",
    "            if word: #relevant term\n",
    "                none_counter = 0 #restart counter\n",
    "\n",
    "                #set current virus \n",
    "                if word.startswith('v_'):\n",
    "                    #different virus\n",
    "                    if word != 'v_unk' and word != bundle[0]:\n",
    "                        #save current result if hitting a different virus\n",
    "                        if bundle[0] and bundle[0] != 'v_unk':\n",
    "                            #skip modifier\n",
    "                            if modifier[3]:\n",
    "                                modifier[3] = None #reset skip modifier\n",
    "                                bundle[1] = None\n",
    "                            else:\n",
    "                                save(bundle)\n",
    "                                virus_counter += 1 #increase counter if different virus in segment     \n",
    "                        bundle[0] = word\n",
    "                    #same virus\n",
    "                    elif word != 'v_unk' and word == bundle[0]:\n",
    "                        #save current result if there is one\n",
    "                        if bundle[1]:\n",
    "                            save(bundle)\n",
    "                        bundle[0] = word\n",
    "                    #only set to general virus if there's no current virus\n",
    "                    elif word == 'v_unk' and not bundle[0]:\n",
    "                        bundle[0] = word\n",
    "\n",
    "                #set current result\n",
    "                elif word.startswith('r_'):\n",
    "                    if word == 'r_ind':\n",
    "                        if bundle[1]: \n",
    "                            save(bundle)\n",
    "                        bundle[1] = word\n",
    "                    elif word == 'r_neg' and bundle[1] not in ('r_ind',):\n",
    "                        if bundle[1]: \n",
    "                            save(bundle)\n",
    "                        bundle[1] = word\n",
    "                    elif word == 'r_pos' and bundle[1] not in ('r_ind', 'r_neg'):\n",
    "                        bundle[1] = word\n",
    "\n",
    "                    elif word in ('r_rej', 'r_can', 'r_pen') and bundle[1] not in ('r_ind', 'r_neg', 'r_pos'):\n",
    "                        if word == 'r_rej':\n",
    "                            bundle[1] = word\n",
    "                        elif word == 'r_can' and bundle[1] not in ('r_rej',):\n",
    "                            bundle[1] = word\n",
    "                        elif word == 'r_pen' and bundle[1] not in ('r_rej', 'r_can'):\n",
    "                            bundle[1] = word\n",
    "\n",
    "                #set current test\n",
    "                elif word.startswith('t_'):\n",
    "                    bundle[2] = word\n",
    "\n",
    "                #final modifier\n",
    "                elif word == 'final':\n",
    "                    if bundle[0] and bundle[1]:\n",
    "                        save(bundle)\n",
    "                    modifier[0] = True\n",
    "                    bundle[1] = None #reset result\n",
    "\n",
    "                #presumptive modifier\n",
    "                elif word == 'presumptive':\n",
    "                    modifier[1] = True\n",
    "\n",
    "                #end modifier/word\n",
    "                elif word == 'end':\n",
    "                    #end early only if there is already result\n",
    "                    if len([o for o in output if o[1] in ('r_ind', 'r_neg', 'r_pos')]) > 0: \n",
    "                        return output\n",
    "                    if bundle[0] and bundle[1]:\n",
    "                        save(bundle)\n",
    "                    modifier[0:4] = [False, False, True, False] #end modifier skips next save\n",
    "                    #end early only if there is already result\n",
    "                    if len(output) > 0:\n",
    "                        return output\n",
    "\n",
    "                #skip modifier\n",
    "                elif word == 'skip':\n",
    "                    if bundle[0] and bundle[1]:\n",
    "                        save(bundle)\n",
    "                    modifier[3] = True\n",
    "                    bundle[0] = None\n",
    "                    bundle[1] = None\n",
    "\n",
    "                #stop word\n",
    "                elif word == 'stop':\n",
    "                    if bundle[0] and bundle[1]:\n",
    "                        save(bundle)\n",
    "                    modifier[0:4] = [False, False, False, False]\n",
    "                    bundle[0] = None\n",
    "                    bundle[1] = None           \n",
    "\n",
    "                #reset word\n",
    "                elif word == 'reset':\n",
    "                    modifier[0:4] = [False, False, False, False]\n",
    "                    bundle[0] = None\n",
    "                    bundle[1] = None\n",
    "\n",
    "            else: #word is None\n",
    "                none_counter += 1\n",
    "\n",
    "                if none_counter == 2: #can change threshold\n",
    "                    #save if there is current virus and result\n",
    "                    if bundle[0] and bundle[1]:\n",
    "                        save(bundle)\n",
    "                    #save the last virus if multiple were listed\n",
    "                    elif bundle[0] and bundle[0] != 'v_unk' and virus_counter > 1:\n",
    "                        if modifier[3]:\n",
    "                            modifier[3] = None #reset skip modifier\n",
    "                        else:\n",
    "                            save(bundle)\n",
    "                    #reset\n",
    "                    none_counter = 0 \n",
    "                    virus_counter = 0\n",
    "                    bundle[0] = None\n",
    "                    bundle[1] = None\n",
    "                    modifier[0:4] = [False, False, False, False]\n",
    "\n",
    "        #if there is still a remaining result\n",
    "        if bundle[1]: \n",
    "            save(bundle)\n",
    "\n",
    "        #if there is an extra virus listed at the end\n",
    "        elif bundle[0] and bundle[0] != 'v_unk' and virus_counter > 1 and not modifier[3]:\n",
    "            save(bundle)\n",
    "\n",
    "        return output\n",
    "\n",
    "    #using reference excel to assign LOINCs to virus and test type\n",
    "    #added COVID19 LOINCs\n",
    "    xlsx_filename = 'COVID19_Resp_codes_20241231.xls'\n",
    "    mappings = {'--':'unk', 'culture':'cult', 'other':'oth', 'entero_rhino_D68':'entero_rhino'}\n",
    "\n",
    "    df_loincs = pd.read_excel(xlsx_filename, sheet_name='Resp_LOINCs')\n",
    "    df_loincs_covid = pd.read_excel(xlsx_filename, sheet_name='COVID19_LOINCs')\n",
    "    df_loincs = df_loincs.append(df_loincs_covid)\n",
    "\n",
    "    #cleaning the categories to match previously defined ones\n",
    "    df_loincs = df_loincs.replace(mappings)\n",
    "    df_loincs['Virus_to_assign'] = df_loincs['Virus_to_assign'].apply(lambda x: 'coronavirus' if 'corona' in x else x)\n",
    "    df_loincs['Virus_to_assign'] = df_loincs['Virus_to_assign'].apply(lambda x: 'v_' + x)\n",
    "    df_loincs['Test_to_assign'] = df_loincs['Test_to_assign'].apply(lambda x: 't_' + x)\n",
    "\n",
    "    #assign LOINCs to virus and test type\n",
    "    loincs_by_v = {}\n",
    "    loincs_by_t = {}\n",
    "    for index, row in df_loincs.iterrows():\n",
    "        loincs_by_v.setdefault(row['Virus_to_assign'], [])\n",
    "        loincs_by_v[row['Virus_to_assign']].append(row['LOINCs'])\n",
    "        loincs_by_t.setdefault(row['Test_to_assign'], [])\n",
    "        loincs_by_t[row['Test_to_assign']].append(row['LOINCs'])\n",
    "\n",
    "    #remove the unk ones\n",
    "    del loincs_by_v['v_unk']\n",
    "    del loincs_by_t['t_unk']\n",
    "\n",
    "    #use reference excel to assign TR codes to virus and test type\n",
    "    #added COVID19 TR codes\n",
    "    df_tr_codes = pd.read_excel(xlsx_filename, sheet_name='Resp_TRs')\n",
    "    df_tr_covid = pd.read_excel(xlsx_filename, sheet_name='COVID19_TRs')\n",
    "    df_tr_codes = df_tr_codes.append(df_tr_covid)\n",
    "\n",
    "    #cleaning the categories to match previously defined ones\n",
    "    df_tr_codes = df_tr_codes.replace(mappings)\n",
    "    df_tr_codes['Virus_to_assign'] = df_tr_codes['Virus_to_assign'].apply(lambda x: 'coronavirus' if 'corona' in x else x)\n",
    "    df_tr_codes['Virus_to_assign'] = df_tr_codes['Virus_to_assign'].apply(lambda x: 'v_' + x)\n",
    "    df_tr_codes['Test_to_assign'] = df_tr_codes['Test_to_assign'].apply(lambda x: 't_' + x)\n",
    "\n",
    "    #assign LOINCs to virus and test type\n",
    "    tr_codes_by_v = {}\n",
    "    tr_codes_by_t = {}\n",
    "    for index, row in df_tr_codes.iterrows():\n",
    "        tr_codes_by_v.setdefault(row['Virus_to_assign'], [])\n",
    "        tr_codes_by_v[row['Virus_to_assign']].append(row['TRs'])\n",
    "        tr_codes_by_t.setdefault(row['Test_to_assign'], [])\n",
    "        tr_codes_by_t[row['Test_to_assign']].append(row['TRs'])\n",
    "\n",
    "    #remove the unk ones\n",
    "    del tr_codes_by_v['v_unk']\n",
    "    del tr_codes_by_t['t_unk']\n",
    "\n",
    "    # assign more details to v_unk or t_unk based on LOINC and TR code\n",
    "    # group by test type and then type of virus, remove duplicates\n",
    "    loinc_exclusions = ['10219-4','10182-4','11329-0','14869-2','21026-0','22633-2','22634-0','22635-7','22636-5','22637-3',\n",
    "                        '22638-1','22639-9','3150-0','33882-2','35265-8','41000-1','47526-9','49049-0','55752-0','56816-2',\n",
    "                        '59465-5','59466-3','611-4','664-3','66746-9','76425-8','94659-0','XON10007-3','XON10011-5',\n",
    "                        'XON10313-5','XON10315-0','XON10316-8','XON10337-4','XON11913-1','XON12721-7','XON12875-1',\n",
    "                        'XON13543-4','XON13544-2','XON13545-9', 'XON10318-4','XON10335-8','XON12949-4','94558-4','94661-6',\n",
    "                        '43305-2','2075-0','2524-7','29308-4','27112-2','633-8','48807-2','45012-2','94508-9','94762-2',\n",
    "                        'XON13688-7','XON13553-3','608-0','97155-6','24041-6','10466-1','2028-9','60568-3','13316-5',\n",
    "                        'XON10013-1','4544-3','32623-1','6690-2','718-7','751-8','777-3','785-6','786-4','787-2','788-0',\n",
    "                        '789-8','18727-8','57006-9','630-4','XON11903-2','14682-9','77202-0','48815-5','42216-2','26881-3',\n",
    "                        '61377-8','48575-5','90041-5','88636-6','29546-9','65222-2','94652-5','82752-7','91560-3','93848-0',\n",
    "                        '94651-7','XON12106-1', 'XON13682-0', '32367-5','62424-7','4996-5','8665-2','52121-1','67098-4',\n",
    "                        '52973-5','XON11882-8','32355-0','59464-8','42957-1','29723-4','10652-6','2705-2','XON12395-0',\n",
    "                        'XON11914-9','XON13691-1','XON10710-2']\n",
    "    #Adding Loinc\n",
    "    tr_exclusions = ['TR12942-9','TR12952-8','TR12953-6','TR10264-0','TR11823-2','TR10854-8','TR12253-1','TR10487-7',\n",
    "                     'TR11723-4','TR12704-3','TR11935-4','TR12000-6','TR12613-6','TR11443-9','TR12210-1','TR11943-8',\n",
    "                     'TR11721-8','TR10807-6','TR11640-0','TR11673-1','TR12029-5','TR11418-1','TR11976-8','TR10749-0',\n",
    "                     'TR10149-3','TR12259-8','TR11398-5','TR11671-5','TR11882-8','TR10713-6','TR10826-6','TR11763-0',\n",
    "                     'TR10694-8','TR10686-4','TR11557-6','TR11561-8','TR11987-5','TR11933-9','TR10882-9','TR12584-9',\n",
    "                     'TR11390-2','TR10745-8','TR11989-1','TR10540-3']\n",
    "\n",
    "    def process_result(tokens, testrequestcode, observationcode, results):\n",
    "        dd = {}\n",
    "        update_t_pcr_flag = False\n",
    "\n",
    "        ###extra conditions\n",
    "\n",
    "        #remove voc \n",
    "        if any([t in ['voc','vocs','variant','variants'] for t in tokens[0:7]]):\n",
    "            return dd\n",
    "\n",
    "        #LOINC/TR exclusions\n",
    "        if (observationcode in loinc_exclusions) or (testrequestcode in tr_exclusions):\n",
    "            return dd\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            #exclude antibody tests\n",
    "            if tokens[i:i+3] in (['covid','19','igg'],['cov','2','antibodies'],):\n",
    "                return dd\n",
    "            #delete all results for irrelevant phrases\n",
    "            if tokens[i:i+5] in (['swab', 'is', 'required', 'for', 'both'],\n",
    "                                 ['is', 'unable', 'to', 'go', 'until'],\n",
    "                                 ['human','herpes','simplex','virus','type'],\n",
    "                                 ['registration','error','please','disregard','result'],\n",
    "                                 ['was','exposed','to','caregiver','with'],\n",
    "                                 ['corrected','report','specimen','sent','to']):\n",
    "                return dd\n",
    "            #make presumptive-positive if test is investigational\n",
    "            if tokens[i:i+3] in (['not', 'been', 'established'], \n",
    "                                 ['is', 'considered', 'investigational'], \n",
    "                                 ['a', 'retrospective', 'review']):\n",
    "                for r in results:\n",
    "                    if (r[0] == 'v_covid' or r[0] == 'v_unk') and r[1] == 'r_pos':\n",
    "                        r[1] = 'r_pre' \n",
    "            #change negative to pending if there are results to follow\n",
    "            if tokens[i:i+3] == ['to', 'follow', 'tested']:\n",
    "                for r in results:\n",
    "                    if r[1] in ('r_neg','r_can','r_rej') and not r[3]:\n",
    "                        r[1] = 'r_pen'\n",
    "            if tokens[i] == 'naat' or tokens[i:i+4] == ['isothermal', 'nucleic', 'acid', 'amplification'] or tokens[i:i+5] == ['confirmatory', 'pcr', 'testing', 'is', 'required'] or tokens[i:i+3] == ['pcr', 'not', 'collected']:\n",
    "                update_t_pcr_flag = True\n",
    "            if tokens[i:i+2] == ['id', 'now'] and 'detected' in tokens:\n",
    "                update_t_pcr_flag = True\n",
    "\n",
    "        #change negative to indeterminate for indeterminate multiplex\n",
    "        if len(set([v for (v,r,t,f) in results])) > 4:\n",
    "            if ('indeterminate' in tokens[0:3] or results[0][0:2] == ['v_unk','r_ind']): \n",
    "                for r in results:\n",
    "                    if r[1] == 'r_neg':\n",
    "                        r[1] = 'r_ind'\n",
    "            elif ('duplicate' in tokens[0:3]): \n",
    "                for r in results:\n",
    "                    if r[1] == 'r_neg':\n",
    "                        r[1] = 'r_can'\n",
    "\n",
    "        #if there is already a covid result, remove v_unk\n",
    "        if any([(r[0] == 'v_covid' and r[1] in ['r_pos', 'r_pre', 'r_ind', 'r_neg']) for r in results]):\n",
    "            results = [r for r in results if r[0] != 'v_unk']\n",
    "\n",
    "        #remove pcr info if it is a rapid test\n",
    "        if testrequestcode == 'TR12946-0':\n",
    "            if update_t_pcr_flag:\n",
    "                for i in range(len(results)):\n",
    "                    if results[i][2] == 't_pcr':\n",
    "                        results[i][2] = 't_unk' \n",
    "            else:\n",
    "                results = [r for r in results if r[2] != 't_pcr']\n",
    "\n",
    "        ###determine virus or test based on LOINC or TR\n",
    "        v_from_loinc = [loinc_vir for loinc_vir, loincs in loincs_by_v.items() if observationcode in loincs]\n",
    "        v_from_tr = [tr_codes_vir for tr_codes_vir, tr_codes in tr_codes_by_v.items() if testrequestcode in tr_codes]\n",
    "        t_from_loinc = [loinc_test for loinc_test, loincs in loincs_by_t.items() if observationcode in loincs]\n",
    "        t_from_tr = [tr_codes_test for tr_codes_test, tr_codes in tr_codes_by_t.items() if testrequestcode in tr_codes]\n",
    "\n",
    "        #determine if there are any final/interpretation results\n",
    "        viruses_with_final = [v for (v,r,t,f) in results if r in ('r_pos', 'r_pre', 'r_ind', 'r_neg', 'r_rej') and f]\n",
    "        results_final = results\n",
    "        #remove the non-final/interpretation results for viruses with final/interpretation\n",
    "        for vf in viruses_with_final:\n",
    "            results_final = [(v,r,t,f) for (v,r,t,f) in results if not (v in vf and not f)]\n",
    "\n",
    "        for v, r, t, f in results_final:\n",
    "            #fill in unknown virus\n",
    "            if v == 'v_unk':\n",
    "                if len(v_from_loinc) > 0:\n",
    "                    v = v_from_loinc[0]\n",
    "                elif len(v_from_tr) > 0:\n",
    "                    v = v_from_tr[0]\n",
    "\n",
    "            #fill in unknown test\n",
    "            if t == 't_unk':\n",
    "                if len(t_from_loinc) > 0:\n",
    "                    t = t_from_loinc[0]\n",
    "                elif len(t_from_tr) > 0:\n",
    "                    t = t_from_tr[0]\n",
    "\n",
    "            #fill in pcr if there is a pcr term in text\n",
    "            if t == 't_unk' and 'pcr' in tokens: \n",
    "                t = 't_pcr'\n",
    "\n",
    "            #remove unknown virus results\n",
    "            if v != 'v_unk':\n",
    "                v, r, t = v[2:], r[2:], t[2:]\n",
    "                #all tests that aren't pcr are oth\n",
    "                #t = t if t == 'pcr' else 'oth'\n",
    "\n",
    "                #ASSUME EVERYTHING PCR FOR COVID DATASET\n",
    "                t = 'pcr'\n",
    "\n",
    "                dd.setdefault(t, [])\n",
    "\n",
    "                #compiling results with hierarchy: S (presumptive positive) > P (positive) > I (indeterminate) \n",
    "                #                                  > N (negative) > D (pending) > R (invalid) > C (cancelled) \n",
    "                same_vir = False\n",
    "                for i in range(len(dd[t])):\n",
    "                    if v == dd[t][i][0]:\n",
    "                        same_vir = True\n",
    "                        if r == 'pre':\n",
    "                            dd[t][i] = (v,r)\n",
    "                        elif r == 'pos' and dd[t][i][1] not in ('pre',):\n",
    "                            dd[t][i] = (v,r)\n",
    "                        elif r == 'ind' and dd[t][i][1] not in ('pre', 'pos'):\n",
    "                            dd[t][i] = (v,r)\n",
    "                        elif r == 'neg' and dd[t][i][1] not in ('pre', 'pos', 'ind'):\n",
    "                            dd[t][i] = (v,r)\n",
    "                        elif r == 'pen' and dd[t][i][1] not in ('pre', 'pos', 'ind', 'neg'):\n",
    "                            dd[t][i] = (v,r)\n",
    "                        elif r == 'rej' and dd[t][i][1] not in ('pre', 'pos', 'ind', 'neg', 'pen'):\n",
    "                            dd[t][i] = (v,r)\n",
    "                        elif r == 'can':\n",
    "                            pass\n",
    "                if not same_vir:\n",
    "                    dd[t].append((v,r))\n",
    "\n",
    "        return dd\n",
    "\n",
    "    #create output as character value for each virus and test type\n",
    "    result_char = {'pre':'S', 'pos': 'P', 'ind':'I', 'neg':'N', 'pen':'D', 'can':'C', 'rej':'R'}\n",
    "\n",
    "    def char_output(results, ind):\n",
    "\n",
    "        #loop through each test type and virus\n",
    "        for t, pairs in results.items(): #need to update if there are multiple test types\n",
    "            for v, r in pairs:\n",
    "                if v in ('adenovirus', 'bocavirus', 'coronavirus', 'entero_rhino', 'hmv', 'covid'):\n",
    "                    df_results.at[ind, v] = result_char[r]\n",
    "\n",
    "                elif v.startswith('para'):\n",
    "                    df_results.at[ind, 'para'] = result_char[r]\n",
    "\n",
    "                elif v.startswith('flu'):\n",
    "                    df_results.at[ind, 'flu'] = result_char[r]\n",
    "                    if '_a' in v:\n",
    "                        df_results.at[ind, 'flu_a'] = result_char[r]\n",
    "                    if '_h1' in v:\n",
    "                        df_results.at[ind, 'flu_a_h1'] = result_char[r]\n",
    "                    if '_h3' in v:\n",
    "                        df_results.at[ind, 'flu_a_h3'] = result_char[r]\n",
    "                    if '_b' in v:\n",
    "                        df_results.at[ind, 'flu_b'] = result_char[r]\n",
    "\n",
    "                elif v.startswith('rsv'):\n",
    "                    df_results.at[ind, 'rsv'] = result_char[r]\n",
    "                    if '_a' in v:\n",
    "                        df_results.at[ind, 'rsv_a'] = result_char[r]\n",
    "                    if '_b' in v:\n",
    "                        df_results.at[ind, 'rsv_b'] = result_char[r]\n",
    "\n",
    "        return\n",
    "\n",
    "    #%%time\n",
    "\n",
    "    #clean text\n",
    "    df_gp[\"cleaned_value\"] = df_gp[\"value\"].apply(clean)\n",
    "\n",
    "    #group by unique records (org, TR code, Obs code, cleaned text) and store original indexes as tuple\n",
    "    df_gp = df_gp.reset_index()\n",
    "    groupby_vars = ['reportinglaborgname', 'testrequestcode', 'observationcode', 'cleaned_value']\n",
    "    df_gp = df_gp.groupby(groupby_vars).agg({'value': 'count', \n",
    "                                                     'original_indexes': lambda x: tuple([i for tup in x for i in tup])}).reset_index()\n",
    "    df_gp = df_gp.rename(columns={'value':'count'})\n",
    "\n",
    "    df_gp = df_gp.sort_values(by=['count'], ascending=False).reset_index(drop=True)\n",
    "    print('unique records after cleaning:', len(df_gp))\n",
    "\n",
    "    #tokenize\n",
    "    df_gp[\"cleaned_tokenized_value\"] = df_gp[\"cleaned_value\"].apply(tokenize)\n",
    "\n",
    "    #assign labels using dictionary\n",
    "    df_gp[\"useful_tokens\"] = df_gp[\"cleaned_tokenized_value\"].apply(assign_labels)\n",
    "\n",
    "    #interpret the labelled tokens\n",
    "    df_gp[\"initial_results\"] = df_gp[\"useful_tokens\"].apply(interpret)\n",
    "\n",
    "    #fill in unknown viruses based on LOINC or TR code, roll up results to one test type\n",
    "    final_results = []\n",
    "    for i in range(len(df_gp)):\n",
    "        final_results.append(process_result(df_gp[\"cleaned_tokenized_value\"][i],\n",
    "                                            df_gp[\"testrequestcode\"][i], df_gp[\"observationcode\"][i], \n",
    "                                            df_gp[\"initial_results\"][i]))\n",
    "\n",
    "    #translate results to 1-character format\n",
    "    col_virus = ['covid', 'adenovirus', 'bocavirus', 'coronavirus', 'flu', 'flu_a', 'flu_a_h1', 'flu_a_h3', 'flu_b',\n",
    "             'entero_rhino', 'hmv', 'para', 'rsv', 'rsv_a', 'rsv_b']\n",
    "\n",
    "    #create empty df to fill in results\n",
    "    df_results = pd.DataFrame(index=np.arange(len(df_gp)), columns=['original_indexes']+col_virus)\n",
    "    df_results['original_indexes'] = df_gp['original_indexes']\n",
    "\n",
    "    #fill in results\n",
    "    for i in range(len(df_gp)):\n",
    "        char_output(final_results[i], i)\n",
    "\n",
    "    output = [None]*len(df)\n",
    "\n",
    "    #order results based on original_indexes\n",
    "    for row in df_results.itertuples():\n",
    "        for i in row[1]: #original_indexes\n",
    "            output[i] = tuple(row[2:])\n",
    "\n",
    "    if output_flag == 1:                \n",
    "        df_output = pd.concat([df, pd.DataFrame(output, columns=col_virus)], axis=1)\n",
    "\n",
    "    elif output_flag == 2: \n",
    "        df_output = df_raw.join(df[['exclude_flag']].join(pd.DataFrame(output, columns=col_virus)))\n",
    "\n",
    "    else:\n",
    "        print('PLEASE ENTER ONE OF THE FOLLOWING OPTIONS FOR OUTPUT_FLAG IN THE FIRST CELL: 1, 2')\n",
    "\n",
    "    #FINAL DATASET TO OUTPUT\n",
    "    df_output.to_csv(f'{output_filename}_{batch}.csv', index=False)\n",
    "    df_output['covid'].value_counts()\n",
    "    #df_output.describe()\n",
    "\n",
    "    #tracker for unique records (some records may be marked as new if clean function changes)\n",
    "\n",
    "    #initialize tracker\n",
    "    try:\n",
    "        f = open('record_tracker.pkl')\n",
    "        f.close()\n",
    "    except FileNotFoundError:\n",
    "        df_tracker = pd.DataFrame(columns=['filename', 'reportinglaborgname', 'testrequestcode', 'observationcode', 'cleaned_value'])\n",
    "        df_tracker.to_pickle(\"./record_tracker.pkl\")\n",
    "        print('CREATING RECORD TRACKER FILE')\n",
    "\n",
    "    #read tracker\n",
    "    df_tracker = pd.read_pickle('./record_tracker.pkl')\n",
    "\n",
    "    #RESET TRACKER\n",
    "    #df_tracker = df_tracker.iloc[0:0]\n",
    "\n",
    "    df_tracker_orig = df_tracker[['reportinglaborgname', 'testrequestcode', 'observationcode', 'cleaned_value']].copy(deep = True)\n",
    "    df_tracker_delta = df_gp[['reportinglaborgname', 'testrequestcode', 'observationcode', 'cleaned_value']].copy(deep = True)\n",
    "\n",
    "    #set difference\n",
    "    df_tracker_delta = pd.concat([df_tracker_delta, df_tracker_orig, df_tracker_orig], ignore_index=True).drop_duplicates(keep=False)\n",
    "    print('Original tracker length:', len(df_tracker_orig))\n",
    "    print('Delta tracker length:', len(df_tracker_delta))\n",
    "\n",
    "\n",
    "    #intermediate output for checking results\n",
    "    int_output_cols = ['count', 'reportinglaborgname', 'testrequestcode', 'observationcode', 'cleaned_value']\n",
    "    df_gp[int_output_cols].join(df_results.drop(columns=['original_indexes'])).to_csv(f'intermediate_output_{batch}.csv')\n",
    "    df_gp[int_output_cols][df_gp.index.isin(df_tracker_delta.index)].join(df_results.drop(columns=['original_indexes'])).to_csv(f'intermediate_output_delta_{batch}.csv')\n",
    "\n",
    "    #Extract new TR codes and LOINCs\n",
    "    df_code_og_tr = df_tracker['testrequestcode'].drop_duplicates()\n",
    "    df_code_og_loinc = df_tracker['observationcode'].drop_duplicates()\n",
    "    df_code_new_tr = df_tracker_delta['testrequestcode'].drop_duplicates()\n",
    "    df_code_new_loinc = df_tracker_delta['observationcode'].drop_duplicates()\n",
    "    print(df_code_new_tr[~df_code_new_tr.isin(df_code_og_tr)])\n",
    "    print(df_code_new_loinc[~df_code_new_loinc.isin(df_code_og_loinc)])\n",
    "\n",
    "    #FINALIZE THE RECORD TRACKER (only run when you are satisfied with the review process)\n",
    "    #add filename\n",
    "    df_tracker_delta['filename'] = input_filename\n",
    "\n",
    "    #add the delta\n",
    "    df_tracker = pd.concat([df_tracker, df_tracker_delta], sort=False, ignore_index=True)\n",
    "\n",
    "    #save file\n",
    "    df_tracker.to_pickle(\"./record_tracker.pkl\")\n",
    "    print('Records in tracker:', len(df_tracker))\n",
    "\n",
    "    #cleanup\n",
    "    del df\n",
    "    del df_gp\n",
    "    del df_output\n",
    "    del df_tracker\n",
    "    del df_tracker_orig\n",
    "    del df_tracker_delta\n",
    "    \n",
    "    return current_batch_cut_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-resident",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-fusion",
   "metadata": {},
   "source": [
    "## Parameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of record to be processed in each batch, 30000000 is an example. You can try any number you want\n",
    "batch_size = 30000000\n",
    "\n",
    "##IntegrationPoint03_Open\n",
    "# input path and filename (can be .sas7bdat file or csv file)\n",
    "input_path = '//'\n",
    "\n",
    "# change the date of input file if needed\n",
    "input_filename = '.sas7bdat'\n",
    "\n",
    "# name of patientid variable in input dataset, will be renamed as 'patientid'\n",
    "input_patientid_var = 'ikn'\n",
    "##IntegrationPoint03_Close\n",
    "\n",
    "# output additional columns\n",
    "#1 = with key columns, 2 = with ALL columns\n",
    "output_flag = 1\n",
    "\n",
    "# output filename\n",
    "output_filename = 'output'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-youth",
   "metadata": {},
   "source": [
    "## Validation: check if the input file is sorted by ordersid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSorted(file_path, file_name, col_name, descending=True):\n",
    "    if file_name.endswith('.csv'):\n",
    "        ##IntegrationPoint05_Open\n",
    "        ordersid_ls = pd.read_csv(file_path+file_name)[col_name].values.tolist()\n",
    "        ##IntegrationPoint05_Close\n",
    "    elif input_filename.endswith('.sas7bdat'):\n",
    "        df, _ = pyreadstat.read_sas7bdat(filename_path=file_path+file_name)\n",
    "        ordersid_ls = df[col_name].values.tolist()\n",
    "        del df\n",
    "    else:\n",
    "        raise Exception(\"incorrect file format\")\n",
    "    is_sorted = all(a >= b for a, b in zip(ordersid_ls, ordersid_ls[1:])) if descending else all(a <= b for a, b in zip(ordersid_ls, ordersid_ls[1:]))\n",
    "    del ordersid_ls\n",
    "    return is_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the input file is sorted by ordersid descending, delete \"False\". If ascending, you can keep it as it is\n",
    "is_sorted = checkSorted(input_path, input_filename, 'ordersid', False)\n",
    "if (not is_sorted) :\n",
    "    print(\"==================================================================================\")\n",
    "    print(\"==================================================================================\")\n",
    "    print(\"   Your file is not sorted. Please sort your file and restart from the top.\")\n",
    "    print(\"==================================================================================\")\n",
    "    print(\"==================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-nothing",
   "metadata": {},
   "source": [
    "## Process Data in Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-nature",
   "metadata": {},
   "source": [
    "#### If you get assertion error, please don't proceed. Please sort your input file by ordersid and restart from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-curtis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#call processBatch\n",
    "i = 0\n",
    "last_batch_cut_index = 0\n",
    "actual_batch_size = []\n",
    "while True:\n",
    "    start = time.time()\n",
    "    current_batch_cut_index = processBatch(i, last_batch_cut_index, batch_size, input_path, input_filename, input_patientid_var, output_filename, output_flag)\n",
    "    actual_batch_size.append(current_batch_cut_index - last_batch_cut_index)\n",
    "    last_batch_cut_index = current_batch_cut_index\n",
    "    if last_batch_cut_index == -1:\n",
    "        break\n",
    "    end = time.time()\n",
    "    print(f'---------------------- finished batch {i} ----------------------')\n",
    "    print(f'------------------- processed to row {last_batch_cut_index} -------------------')\n",
    "    print(f'------------- processing time for this batch is {round(end - start, 2)} s -------------\\n\\n')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-photograph",
   "metadata": {},
   "source": [
    "## Combine Batch Outputs to the Final Output\n",
    "#### When running this part it may have some warning messages talking about different types, you can ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when run seperately, you can define total_batch_num as \n",
    "TOTAL_BATCH_NUM = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the complete output.csv\n",
    "if os.path.exists(f'{input_path}{output_filename}.csv'):\n",
    "    os.remove(f'{input_path}{output_filename}.csv')\n",
    "\n",
    "output_files = [f'{output_filename}_{i}.csv' for i in range(TOTAL_BATCH_NUM)]\n",
    "\n",
    "pd.read_csv(f'{input_path}{output_files[0]}', index_col=None, nrows=0).to_csv(input_path + f'{output_filename}.csv', mode=\"a\", index=False)\n",
    "\n",
    "for i in range(TOTAL_BATCH_NUM):\n",
    "    chunks = pd.read_csv(f'{input_path}{output_files[i]}', index_col=None, chunksize=actual_batch_size[i], dtype={'patientid': object, 'ordersid':float})\n",
    "    for chunk in chunks:\n",
    "        chunk.to_csv(f'{input_path}{output_filename}.csv', mode=\"a\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate_output.csv\n",
    "if os.path.exists(f'{input_path}intermediate_output.csv'):\n",
    "    os.remove(f'{input_path}intermediate_output.csv')\n",
    "\n",
    "intermediate_output_files = [f'intermediate_output_{i}.csv' for i in range(TOTAL_BATCH_NUM)]\n",
    "\n",
    "pd.read_csv(f'{input_path}{intermediate_output_files[0]}', index_col=None, nrows=0).to_csv(input_path + f'intermediate_output.csv', mode=\"a\", index=False)\n",
    "\n",
    "for f in intermediate_output_files:\n",
    "    chunk = pd.read_csv(f'{input_path}{f}', index_col=None)\n",
    "    chunk.to_csv(f'{input_path}intermediate_output.csv', mode=\"a\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate_output_delta.csv\n",
    "if os.path.exists(f'{input_path}intermediate_output_delta.csv'):\n",
    "    os.remove(f'{input_path}intermediate_output_delta.csv')\n",
    "\n",
    "intermediate_output_delta_files = [f'intermediate_output_delta_{i}.csv' for i in range(TOTAL_BATCH_NUM)]\n",
    "\n",
    "pd.read_csv(f'{input_path}{intermediate_output_delta_files[0]}', index_col=None, nrows=0).to_csv(input_path + f'intermediate_output_delta.csv', mode=\"a\", index=False)\n",
    "\n",
    "for f in intermediate_output_delta_files:\n",
    "    chunk = pd.read_csv(f'{input_path}{f}', index_col=None)\n",
    "    chunk.to_csv(f'{input_path}intermediate_output_delta.csv', mode=\"a\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-navigator",
   "metadata": {},
   "source": [
    "## Remove Batch Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in output_files:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "assert(not os.path.exists(output_files[0]))\n",
    "\n",
    "for f in intermediate_output_files:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "assert(not os.path.exists(intermediate_output_files[0]))\n",
    "\n",
    "for f in intermediate_output_delta_files:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "assert(not os.path.exists(intermediate_output_delta_files[0]))\n",
    "\n",
    "##IntegrationPoint04_Open\n",
    "##IntegrationPoint04_Close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-adobe",
   "metadata": {},
   "source": [
    "# Algorithm description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-furniture",
   "metadata": {},
   "source": [
    "Using the useful_tokens field, this interpret function sequentially \"reads\" the terms. It picks up virus/result/test terms and they are held in a \"bundle\" (virus, result, test). There are also multiple modifiers that affect the way that the algorithm processes the terms. These modifiers are: final (flag to take highest priority later on), presumptive (change pos to pre), end (end reading early or skip the next save), and skip (skip the 'save when virus switches' rule once). Any time a bundle is saved, the bundle (except for test type) and the final/presumptive modifiers are cleared. If a save occurs with incomplete information, the virus defaults to an unknown virus, result defaults to negative, and test defaults to unknown test. Whenever a save happens, all of the previous tokens+labels that were read are considered to be a \"segment\".\n",
    "<br>\n",
    "- First, the xml field is processed if there is one. If a relevant virus is found, it is treated as a positive and the bundle is saved.\n",
    "- Next, the algorithm will go through the labelled tokens one by one. There are different conditions for storing terms and saving the bundle when encountering a virus, a result, a special term, or an irrelevant (unlabelled) term.\n",
    "    - Viruses: A relevant virus is always kept. If the virus switches, save the bundle (note: can be affected by skip modifier). If the same virus is read, save the bundle only if there is a result as well. An unknown virus is only kept if there is no current virus.\n",
    "    - Results: A clear result (ind, neg, pos) is kept with hierarchy ind > neg > pos such that a neg/ind can overwrite a positive if it's close together (e.g., \"not detected\" becomes a neg). An unclear result (rej, can, pen) is only kept if there is no current result with hierarchy rej > can > pen. If there is already a previous result and a neg/ind is encounter, save the bundle.\n",
    "    - Special terms:\n",
    "        - Final: Modifier to add flag when saving to specify whether it is a final result, which takes higher priority over all others in the process_result function. Save if there is a current virus and current result. Clear the current result.\n",
    "        - Presumptive: Modifier to change positive (r_pos) into presumptive-positive (r_pre).\n",
    "        - End: Modifier to skip the next save. Save if there is a current virus and current result. Stop the reading if there are any results.\n",
    "        - Skip: Modifier to skip the 'save when virus switches' once. This is reset when a virus switches and the current virus is skipped. Save if there is a current virus and current result. Clear the bundle.\n",
    "        - Reset: Clear bundle without saving.\n",
    "        - Stop: Save if there is a current virus and current result. Clear the bundle.        \n",
    "    - Irrelevant terms: If two irrelevant terms (Nones) are read in a row, save the bundle if there is both a current result and virus. Also save the bundle if there is a virus and the past segment had another virus (virus_counter > 1; normally viruses tested are listed in a mpx or pcr assay). Otherwise, clear the bundle without saving and reset all the counter variables (i.e., start a new segment).\n",
    "    - If the sentence ends before hitting two Nones, save any result and save the bundle if there is a virus and the past segment had another virus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
